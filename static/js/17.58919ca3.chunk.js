(this.webpackJsonpapp=this.webpackJsonpapp||[]).push([[17],{103:function(e,t,i){"use strict";i.d(t,"a",(function(){return p}));var s=i(0),a=i.n(s),n=(i(3),i(43)),r=function(){return(r=Object.assign||function(e){for(var t,i=1,s=arguments.length;i<s;i++)for(var a in t=arguments[i])Object.prototype.hasOwnProperty.call(t,a)&&(e[a]=t[a]);return e}).apply(this,arguments)};var c="",o=null,l=null,h=null;function d(){c="",null!==o&&o.disconnect(),null!==l&&(window.clearTimeout(l),l=null)}function b(e){return["BUTTON","INPUT","SELECT","TEXTAREA"].includes(e.tagName)&&!e.hasAttribute("disabled")||["A","AREA"].includes(e.tagName)&&e.hasAttribute("href")}function j(){var e=null;if("#"===c)e=document.body;else{var t=c.replace("#","");null===(e=document.getElementById(t))&&"#top"===c&&(e=document.body)}if(null!==e){h(e);var i=e.getAttribute("tabindex");return null!==i||b(e)||e.setAttribute("tabindex",-1),e.focus({preventScroll:!0}),null!==i||b(e)||(e.blur(),e.removeAttribute("tabindex")),d(),!0}return!1}function x(e){return a.a.forwardRef((function(t,i){var s="";"string"===typeof t.to&&t.to.includes("#")?s="#"+t.to.split("#").slice(1).join("#"):"object"===typeof t.to&&"string"===typeof t.to.hash&&(s=t.to.hash);var b={};e===n.c&&(b.isActive=function(e,t){return e&&e.isExact&&t.hash===s});var x=function(e,t){var i={};for(var s in e)Object.prototype.hasOwnProperty.call(e,s)&&t.indexOf(s)<0&&(i[s]=e[s]);if(null!=e&&"function"===typeof Object.getOwnPropertySymbols){var a=0;for(s=Object.getOwnPropertySymbols(e);a<s.length;a++)t.indexOf(s[a])<0&&Object.prototype.propertyIsEnumerable.call(e,s[a])&&(i[s[a]]=e[s[a]])}return i}(t,["scroll","smooth","timeout","elementId"]);return a.a.createElement(e,r({},b,x,{onClick:function(e){var i;d(),c=t.elementId?"#"+t.elementId:s,t.onClick&&t.onClick(e),""===c||e.defaultPrevented||0!==e.button||t.target&&"_self"!==t.target||e.metaKey||e.altKey||e.ctrlKey||e.shiftKey||(h=t.scroll||function(e){return t.smooth?e.scrollIntoView({behavior:"smooth"}):e.scrollIntoView()},i=t.timeout,window.setTimeout((function(){!1===j()&&(null===o&&(o=new MutationObserver(j)),o.observe(document,{attributes:!0,childList:!0,subtree:!0}),l=window.setTimeout((function(){d()}),i||1e4))}),0))},ref:i}),t.children)}))}var p=x(n.b);x(n.c)},105:function(e,t,i){},109:function(e,t,i){"use strict";var s=i(0),a=i(5),n=i(43),r=i(166),c=i(226),o=i(514),l=i(515),h=(i(105),i(7));t.a=function(e){var t=e.title,i=e.githubURL,d=e.projectRoute,b=e.projectLink,j=e.thumb,x=Object(a.h)().pathname;return Object(s.useEffect)((function(){window.scrollTo(0,0)}),[]),Object(h.jsxs)(h.Fragment,{children:[Object(h.jsx)("div",{style:{marginTop:"-3rem",backgroundImage:"url(".concat(j,")"),backgroundPosition:"center",backgroundSize:"cover",backgroundRepeat:"no-repeat",height:"100vh",zIndex:-1}}),Object(h.jsxs)("div",{className:"project-home-wrapper",style:{position:"absolute",width:"100%",top:"101vh",left:"0px",transform:"translate(0, -100%)"},children:[Object(h.jsxs)("header",{className:"home-header",children:[Object(h.jsx)("h1",{id:"title",style:{display:"inline-block"},children:t}),Object(h.jsxs)("span",{style:{padding:"0 1em",display:"inline-block"},children:[void 0!=i?Object(h.jsx)(r.a,{title:"View on Github",placement:"bottom",children:Object(h.jsx)("a",{href:i,target:"_blank",children:Object(h.jsx)(o.a,{className:"title-icon"})})}):null,void 0!=d?Object(h.jsx)(r.a,{title:"View project",placement:"bottom",children:Object(h.jsx)(n.b,{to:x+d,children:Object(h.jsx)(l.a,{className:"title-icon"})})}):null,void 0!=b?Object(h.jsx)(r.a,{title:"View project",placement:"bottom",children:Object(h.jsx)("a",{href:b,target:"_blank",children:Object(h.jsx)(l.a,{className:"title-icon"})})}):null]}),Object(h.jsx)(c.a,{style:{borderTopWidth:"1px",borderTopColor:"#000000",opacity:.5}})]}),Object(h.jsx)("div",{style:{height:"8em"}})]})]})}},110:function(e,t,i){"use strict";var s=i(106),a=i.n(s),n=i(109),r=i(18),c=i(509),o=i(508),l=i(0),h=i(7),d=function(e){var t=e.filter((function(e){var t=e.nodeName;return"title"!==e.id&&("H1"===t||"H2"===t||"H3"===t)})),i=-1,s=t.map((function(e){return i++,{key:"contents_".concat(i),href:"#".concat(e.id),title:e.innerHTML,level:parseInt(e.nodeName.slice(-1))}})),a=[],n=[],r=[a];return s.forEach((function(e){var t=n.findIndex((function(t){return t>=e.level}));-1===t?t=n.push(e.level)-1:n.length=t+1,r[t].push(Object.assign({},e,{children:r[t+1]=[]}))})),a},b=function(e){var t=e.title,i=function(){var e=Object(l.useState)([]),t=Object(r.a)(e,2),i=t[0],s=t[1];return Object(l.useEffect)((function(){var e=Array.from(document.querySelectorAll("h1, h2, h3")),t=d(e);s(t)}),[]),{nestedHeadings:i}}().nestedHeadings,s=function(){var e=Object(l.useState)(0),t=Object(r.a)(e,2),i=t[0],s=t[1];return Object(l.useEffect)((function(){var e=document.getElementById("main-navbar");console.log(e),s(e.offsetHeight)}),[]),{navHeight:i}}().navHeight,a=Object(l.useState)("85vh"),n=Object(r.a)(a,2),b=n[0],j=n[1];return Object(l.useEffect)((function(){var e=document.getElementById("toc-breadcrumb");e&&j("calc(100vh - 6rem - ".concat(e.offsetHeight,"px)"))}),[]),Object(h.jsxs)(h.Fragment,{children:[Object(h.jsx)(c.a,{id:"toc-breadcrumb",style:{paddingBottom:"14px",position:"sticky"},items:[{title:Object(h.jsx)("a",{href:"#home",children:"Portfolio"})},{title:Object(h.jsx)("a",{href:"#projects",children:"Projects"})},{title:"".concat(t)}]}),Object(h.jsx)(o.a,{style:{maxHeight:b,overflow:"auto"},targetOffset:s,onClick:function(e,t){e.preventDefault()},items:i})]})},j=i(142),x=i(495),p=i(496),m=j.a.useBreakpoint;t.a=function(e){var t=e.title,i=e.thumb,s=e.projectLink,r=e.projectRoute,c=e.githubURL,o=e.footer,l=e.children,d=m();return Object(h.jsx)(h.Fragment,{children:Object(h.jsxs)(a.a,{children:[Object(h.jsx)(n.a,{title:t,thumb:i,projectRoute:r,projectLink:s,githubURL:c}),Object(h.jsxs)(x.a,{gutter:0,children:[Object(h.jsx)(p.a,{xs:0,lg:5,children:Object(h.jsx)("div",{className:"project-toc-wrapper",children:Object(h.jsx)(b,{title:t})})}),Object(h.jsxs)(p.a,{xs:24,lg:19,children:[Object(h.jsx)("div",{className:"project-content-wrapper",style:{marginRight:d.lg?"17.5vw":"6vw",marginLeft:d.lg?0:"6vw"},children:l}),Object(h.jsx)("div",{className:"project-footer-wrapper",style:{display:"flex",justifyContent:"center",marginTop:"8vh",marginBottom:"5vh",marginRight:d.lg?"17.5vw":"6vw",marginLeft:d.lg?0:"6vw"},children:o?{footer:o}:"\u274b That's all! Thanks for reading. \u274b"})]})]})]})})}},225:function(e,t,i){"use strict";i.r(t);i(0);var s=i(103),a=i(504),n=i(495),r=i(496),c=i(226),o=i.p+"static/media/Canny-Process.86877d85.png",l=i.p+"static/media/Canny-lerp.f05b4606.png",h=(i(130),i(131)),d=i(146),b=i(110),j=i(7);t.default=function(){return Object(j.jsxs)(b.a,{title:d.a.title,thumb:d.a.thumb,projectRoute:"/main",githubURL:"https://github.com/Skittss/PortfolioWebsite/tree/main/src/projects/Webgl-Canny",children:[Object(j.jsx)("h1",{id:"overview",className:"raleway-title",children:"Overview"}),Object(j.jsxs)("div",{style:{paddingBottom:"20px",textAlign:"center"},children:[Object(j.jsx)(a.a,{src:o,fallback:"Canny Process Image"}),Object(j.jsx)("p",{style:{fontSize:"0.75em"},children:Object(j.jsx)("i",{children:"(Canny Edge Detection Steps) "})})]}),Object(j.jsxs)("p",{children:["Canny edge detection is a quintessential image processing algorithm which finds edges in images using a multi-stage process.",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),'This project was created off the back of a similar (prototype) project of mine which drew images via a complex fourier series. You can find that project as "Fourier Sketcher" in the projects tab or the page itself ',Object(j.jsx)("a",{href:"https://skittss.github.io/FourierSketcher/",target:"_blank",children:" here"}),". My implementation of edge detection in this prototype was ",Object(j.jsx)("i",{children:"painfully"})," slow, being CPU bound, and only running on a single core! :(",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"This project is my attempt to make a faster (real-time!) canny edge detection implementation which runs in parallel, on a GPU!",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"In order to achieve this (and show it off on this website), I opted to use WebGL. WebGL comes with its own slew of problems which i will touch on in relevant sections, however, it is still good enough to allow me to write GPU compatible programs (shaders) in GLSL. I also made use of two other libraries to make my life easier:"]}),Object(j.jsxs)("div",{style:{paddingLeft:"3em",paddingRight:"3em"},children:[Object(j.jsxs)(n.a,{style:{display:"flex",alignItems:"center",paddingBottom:"10px"},children:[Object(j.jsx)(r.a,{flex:"1",style:{paddingRight:"10px"},children:Object(j.jsx)("b",{children:"Three.js"})}),Object(j.jsx)(r.a,{flex:"4",children:"A WebGL wrapper which allows for easy writing of fragment shaders. Each step of the process below was written as a fragment shader in some capacity."})]}),Object(j.jsxs)(n.a,{style:{display:"flex",alignItems:"center"},children:[Object(j.jsx)(r.a,{flex:"1",style:{paddingRight:"10px"},children:Object(j.jsx)("b",{children:"React-three-fiber"})}),Object(j.jsxs)(r.a,{flex:"4",children:["An interface between ",Object(j.jsx)("i",{children:"Three.js"})," and ",Object(j.jsx)("i",{children:"React"})," which facilitates creating Three canvases in jsx/ts syntax."]})]})]}),Object(j.jsx)("br",{}),Object(j.jsx)("p",{children:"Canny edge detection is a 6-step process (7 including my own final clean-up):"}),Object(j.jsxs)("p",{style:{paddingLeft:"3em",paddingRight:"3em"},children:[Object(j.jsx)(s.a,{smooth:!0,to:"#grayscale",children:Object(j.jsx)("b",{children:"1. \xa0 Grayscale Encoding"})}),Object(j.jsx)("br",{}),Object(j.jsx)(s.a,{smooth:!0,to:"#blur",children:Object(j.jsx)("b",{children:"2. \xa0 Noise filtering (Gaussian Blur)"})}),Object(j.jsx)("br",{}),Object(j.jsx)(s.a,{smooth:!0,to:"#sobel",children:Object(j.jsx)("b",{children:"3. \xa0 Image Gradient Calculation"})}),Object(j.jsx)("br",{}),Object(j.jsx)(s.a,{smooth:!0,to:"#nms",children:Object(j.jsx)("b",{children:"4. \xa0 Non-maximum suppression"})}),Object(j.jsx)("br",{}),Object(j.jsx)(s.a,{smooth:!0,to:"#threshold",children:Object(j.jsx)("b",{children:"5. \xa0 Double thresholding"})}),Object(j.jsx)("br",{}),Object(j.jsx)(s.a,{smooth:!0,to:"#hysteresis",children:Object(j.jsx)("b",{children:"6. \xa0 Edge tracking (via hysteresis)"})}),Object(j.jsx)("br",{}),Object(j.jsx)(s.a,{smooth:!0,to:"#cleanup",children:Object(j.jsx)("b",{children:Object(j.jsx)("i",{children:"7. \xa0 Final clean-up"})})})]}),Object(j.jsx)("p",{children:"Each step is implemented as a Three.js post-processing shader pass."}),Object(j.jsx)("br",{}),Object(j.jsx)(c.a,{style:{borderTopWidth:"1px",borderTopColor:"#000000",opacity:.5}}),Object(j.jsx)("h1",{id:"grayscale",className:"raleway-title",children:"Grayscale Encoding"}),Object(j.jsxs)("p",{children:["The target image is first converted to grayscale. There is no specific reason to do this except for simplicity; this way we do not have to deal with multiple colour channels later on.",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"An image can be converted to grayscale by setting the value of each colour channel to some weighted average of each individual channel. Different coefficients for the colour channels will allow different details to be preserved in the grayscale image.",Object(j.jsx)("br",{})]}),Object(j.jsxs)("div",{style:{paddingLeft:"3em",paddingRight:"3em"},children:[Object(j.jsxs)(n.a,{style:{display:"flex",alignItems:"center",paddingBottom:"10px"},children:[Object(j.jsx)(r.a,{flex:"200px",children:Object(j.jsx)("b",{children:"BT.601"})}),Object(j.jsx)(r.a,{flex:"auto",children:Object(j.jsx)(h.a,{children:"$R:0.2990,\\ G:0.5870,\\ B:0.1140$."})})]}),Object(j.jsxs)(n.a,{style:{display:"flex",alignItems:"center",paddingBottom:"10px"},children:[Object(j.jsx)(r.a,{flex:"200px",children:Object(j.jsx)("b",{children:"BT.709"})}),Object(j.jsx)(r.a,{flex:"auto",children:Object(j.jsx)(h.a,{children:"$R:0.2126,\\ G:0.7152,\\ B:0.0722$."})})]}),Object(j.jsxs)(n.a,{style:{display:"flex",alignItems:"center",paddingBottom:"10px"},children:[Object(j.jsx)(r.a,{flex:"200px",children:Object(j.jsx)("b",{children:"BT.2100"})}),Object(j.jsxs)(r.a,{flex:"auto",children:[Object(j.jsx)(h.a,{children:"$R:0.2627,\\ G:0.6780,\\ B:0.0593$."})," \xa0 (Typically used for HDR content)"]})]}),Object(j.jsxs)(n.a,{style:{display:"flex",alignItems:"center",paddingBottom:"10px"},children:[Object(j.jsx)(r.a,{flex:"200px",children:Object(j.jsx)("b",{children:"Mean"})}),Object(j.jsxs)(r.a,{flex:"auto",children:[Object(j.jsx)(h.a,{children:"$R:0.3333,\\ G:0.3333,\\ B:0.3333$."})," \xa0 (Simple approach)"]})]})]}),Object(j.jsxs)("p",{children:[Object(j.jsx)("br",{}),"This can be easily achieved in a fragment shader with a uniform vec3 (weights) of these coefficients:",Object(j.jsx)("br",{}),Object(j.jsx)(h.a,{children:"$\\text{gl\\_FragColor}=\\text{vec4}(\\text{weights}.r \\cdot \\text{texel}.r, \\ \\text{weights}.g\\cdot \\text{texel}.g, \\ \\text{weights}.b\\cdot \\text{texel}.b, \\text{texel}.a )$"})]}),Object(j.jsx)("br",{}),Object(j.jsx)(c.a,{style:{borderTopWidth:"1px",borderTopColor:"#000000",opacity:.5}}),Object(j.jsx)("h1",{id:"blur",className:"raleway-title",children:"Noise filtering (Gaussian Blur)"}),Object(j.jsxs)("p",{children:["Next, we can choose to remove noise from the input image with a blur. This way the 'imperfections' - noise - gets smoothed out and is thus at least partially removed from the image.",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"A quick way of computing a blur is with a gaussian distribution. We perform a convolution with a gaussian kernel which effectively adds up pixels around the centre pixel, weighted by their distance from the centre. (Those closer to the centre are weighted more). We can effect how much these pixels are weighted with a parameter",Object(j.jsx)(h.a,{children:"$\\ \\sigma$"})," when we generate the gaussian kernel. (",Object(j.jsx)(h.a,{children:"$\\sigma$"})," effects the standard deviation of a gaussian distribution).",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"A gaussian kernel can be written as:",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),Object(j.jsx)("div",{style:{paddingLeft:"3em",paddingRight:"3em"},children:Object(j.jsx)(h.a,{children:"$\\large G(x,y)= \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{x^2+y^2}{2\\sigma^2}}$"})}),Object(j.jsx)("br",{}),"where ",Object(j.jsx)(h.a,{children:"$x$"})," and ",Object(j.jsx)(h.a,{children:"$y$"})," are the coordinate offsets from the centre of the kernel, and ",Object(j.jsx)(h.a,{children:"$\\sigma$"})," a non-zero value of our choice.",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"There are a few important properties of the kernel above.",Object(j.jsx)("br",{}),Object(j.jsx)("br",{})]}),Object(j.jsxs)("div",{style:{paddingLeft:"3em",paddingRight:"3em"},children:[Object(j.jsxs)(n.a,{style:{display:"flex",paddingBottom:"10px"},children:[Object(j.jsx)(r.a,{style:{paddingTop:"2px"},children:Object(j.jsx)("b",{children:"1. \xa0 \xa0 \xa0"})}),Object(j.jsxs)(r.a,{flex:"1vw",style:{paddingLeft:"20px"},children:[Object(j.jsx)(h.a,{children:"$\\displaystyle\\sum_{x,y}G(x,y) = 1$"}),Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"The sum of the entire kernel ",Object(j.jsx)("i",{children:"must"})," be one. If it were not, we would be brightening or dimming the image with each pass. To ensure this is true, we multiply the entire kernel by a normalization coefficient, which is simply one over the sum of the non-normalized kernel."]})]}),Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),Object(j.jsxs)(n.a,{style:{display:"flex",paddingBottom:"10px"},children:[Object(j.jsx)(r.a,{style:{paddingTop:"10px"},children:Object(j.jsx)("b",{children:"2. \xa0 \xa0 \xa0"})}),Object(j.jsxs)(r.a,{flex:"1vw",style:{paddingLeft:"20px"},children:[Object(j.jsx)(h.a,{children:"$\\large G(x,y)= G(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{x^2}{2\\sigma^2}} \\cdot G(y)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{y^2}{2\\sigma^2}}$"}),Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"The kernel is separable. This means that we can achieve the same convolution as the full kernel by sequentially passing two one-dimensional 'kernels' instead ",Object(j.jsx)(h.a,{children:"$\\big[G(x) \\ \\& \\ G(y)\\big]$"}),". This proves very useful, as it allows the time complexity of the pass to be reduced from ",Object(j.jsx)(h.a,{children:"$O(N^2)$"})," to ",Object(j.jsx)(h.a,{children:"$O(N)$"}),". This is quite a substantial improvement!"]})]}),Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),Object(j.jsxs)(n.a,{style:{display:"flex",paddingBottom:"10px"},children:[Object(j.jsx)(r.a,{children:Object(j.jsx)("b",{children:"3. \xa0 \xa0 \xa0"})}),Object(j.jsxs)(r.a,{flex:"1vw",style:{paddingLeft:"20px"},children:[Object(j.jsx)(h.a,{children:"$G(x) = G(y)^T, \\ G(n) = G(-n)$"}),Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"These are properties which allow us to store only ",Object(j.jsx)("i",{children:"parts"})," of the entire kernel to save on memory. These are both derrived from the fact that the gaussian distribution which the kernel is generated from is symmetrical about ",Object(j.jsx)(h.a,{children:"$0$"})," in both the ",Object(j.jsx)(h.a,{children:"$x$"})," and ",Object(j.jsx)(h.a,{children:"$y$"})," axes, and has rotational symmetry when mapping one axis to the other."]})]})]}),Object(j.jsx)("br",{}),Object(j.jsxs)("p",{children:["Finally we have two parameters for the blur, ",Object(j.jsx)(h.a,{children:"$r$"})," and ",Object(j.jsx)(h.a,{children:"$\\sigma$"}),". Sigma was discussed above, and ",Object(j.jsx)(h.a,{children:"$r$ "}),"is simply the size of the kernel - the interval which ",Object(j.jsx)(h.a,{children:"$x$"})," and ",Object(j.jsx)(h.a,{children:"$y$"})," are taken from: \xa0 ",Object(j.jsx)(h.a,{children:"$(x,y)\\in [-r,r]\\subset\\Z$"}),Object(j.jsx)("br",{}),"More intuitively, ",Object(j.jsx)(h.a,{children:"$r$"})," is the max distance from the centre pixel which we blend other pixels from.",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"Though the maths here is non-trivial, the actual implementation is quite simple. All of the above can be reduced into ~20 lines of JavaScript. You can find this in ",Object(j.jsx)("a",{href:"https://github.com/Skittss/PortfolioWebsite/blob/main/src/projects/Webgl-Canny/src/gaussianKernel.jsx",target:"_blank",children:"gaussianKernel.jsx"})," in the source code."]}),Object(j.jsx)("br",{}),Object(j.jsx)(c.a,{style:{borderTopWidth:"1px",borderTopColor:"#000000",opacity:.5}}),Object(j.jsx)("h1",{id:"sobel",className:"raleway-title",children:"Image Gradient Calculation"}),Object(j.jsxs)("p",{children:["This step gives us most of the information about the actual edges in the image. Subsequent steps exist to refine the information calculated here.",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),'The way edges are detected in this step is by calculating the "gradient" of the image. This might sound a little strange at first - its not everyday that we associate images with calculus, but by phrasing this as "finding the parts of the image where the change in brightness is the greatest" the idea starts to make a bit more sense.',Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"We can calculate the gradient of an image by calculating partial derivatives with respect to ",Object(j.jsx)(h.a,{children:"$x$"})," and ",Object(j.jsx)(h.a,{children:"$y$ "}),"followed by aggregating our results. To do this we can use an ",Object(j.jsx)("i",{children:"operator"})," such as the ",Object(j.jsx)("i",{children:"Sobel operator"})," or ",Object(j.jsx)("i",{children:"Prewitt operator"}),". These operators positively weight values on one side of a centre pixel and negatively on the other in a convolution which as a result calculates the gradient. The ",Object(j.jsx)("i",{children:"Sobel"})," and ",Object(j.jsx)("i",{children:"Prewitt"})," operators are very similar, the only distinction being that ",Object(j.jsx)("i",{children:"Sobel's"})," is slightly biased towards gradients in a cardinal direction, whereas ",Object(j.jsx)("i",{children:"Prewitt's"})," equally weights gradients on the diagonals as well. This can be observed from their kernels: After"]}),Object(j.jsx)("br",{}),Object(j.jsxs)("div",{style:{paddingLeft:"3em",paddingRight:"3em",textAlign:"center"},children:["Sobel:",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),Object(j.jsx)(h.a,{children:"$S_x = \\begin{bmatrix} 1 & 0 & -1 \\\\ 2 & 0 & -2 \\\\ 1 & 0 & -1\\end{bmatrix},\\ S_y = \\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1\\end{bmatrix}$"}),Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"Prewitt:",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),Object(j.jsx)(h.a,{children:"$P_x = \\begin{bmatrix} 1 & 0 & -1 \\\\ 1 & 0 & -1 \\\\ 1 & 0 & -1\\end{bmatrix},\\ P_y = \\begin{bmatrix} 1 & 1 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -1 & -1\\end{bmatrix}$"})]}),Object(j.jsx)("br",{}),Object(j.jsxs)("p",{children:["After perfoming the horizontal and vertical convolutions denoted by ",Object(j.jsx)(h.a,{children:"$G_x, G_y$"}),", we can extract the total gradient magnitude, and its direction (argument/angle) using basic trigonometry and Pythagoras' theorem:"]}),Object(j.jsx)("br",{}),Object(j.jsxs)("div",{style:{paddingLeft:"3em",paddingRight:"3em",textAlign:"center"},children:["Magnitude:",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),Object(j.jsx)(h.a,{children:"$M_{x,y}=\\sqrt{{G_x}^2 + {G_y}^2}$"}),Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"Argument:",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),Object(j.jsx)(h.a,{children:"$A_{x,y}=\\arctan(G_y, G_x)$"})]}),Object(j.jsx)("br",{}),Object(j.jsxs)("p",{children:["The magnitude gives us an indication how strong an edge is at a certain pixel, and the argument indicates which direction the edge lays on at this point.",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"Now, how can we compute this information in WebGL? There are two issues which we have to deal with:"]}),Object(j.jsxs)("div",{style:{paddingLeft:"3em",paddingRight:"3em"},children:[Object(j.jsxs)(n.a,{style:{display:"flex",alignItems:"center",paddingBottom:"10px"},children:[Object(j.jsxs)(r.a,{children:[Object(j.jsx)("b",{children:"1. "})," \xa0\xa0\xa0"]}),Object(j.jsx)(r.a,{flex:"1vw",children:"We need to output two values for a single input texel."})]}),Object(j.jsx)("br",{}),Object(j.jsxs)(n.a,{style:{display:"flex",alignItems:"center"},children:[Object(j.jsxs)(r.a,{children:[Object(j.jsx)("b",{children:"2. "})," \xa0\xa0\xa0"]}),Object(j.jsxs)(r.a,{flex:"1vw",children:["We cannot clamp ",Object(j.jsx)(h.a,{children:"$M_{x,y}$"})," to ",Object(j.jsx)(h.a,{children:"$[0, 1]$"})," as strong edges will clip and not retain precise information about the strength of the edge. This is particularly important for the next step (non-maximum suppression). Similarly, ",Object(j.jsx)(h.a,{children:"$A_{x,y}$"})," cannot be clamped to ",Object(j.jsx)(h.a,{children:"$[0, 1]$"})," either."]})]})]}),Object(j.jsx)("br",{}),Object(j.jsxs)("p",{children:["OpenGL's compute shaders would be ideal for this, however these are not supported by WebGL. Instead, we can use a class included in Three.js - ",Object(j.jsx)("i",{children:"GPUComputationRenderer"})," - to achieve similar behaviour. This class uses a fragment shader in place of the compute shader and ouputs to a floating point texture. We can also add multiple shaders to the renderer which allows us calculate multiple outputs for a single input texture. This solves both problems (albeit in a somewhat hacky way).",Object(j.jsx)("br",{}),"Two final considerations with using this class are:"]}),Object(j.jsxs)("div",{style:{paddingLeft:"3em",paddingRight:"3em"},children:[Object(j.jsxs)(n.a,{style:{display:"flex",alignItems:"center",paddingBottom:"10px"},children:[Object(j.jsxs)(r.a,{children:[Object(j.jsx)("b",{children:"1. "})," \xa0\xa0\xa0"]}),Object(j.jsx)(r.a,{flex:"1vw",children:"We have to manually dispose of all related WebGL components after finished with it to avoid memory leaks."})]}),Object(j.jsx)("br",{}),Object(j.jsxs)(n.a,{style:{display:"flex",alignItems:"center"},children:[Object(j.jsxs)(r.a,{children:[Object(j.jsx)("b",{children:"2. "})," \xa0\xa0\xa0"]}),Object(j.jsxs)(r.a,{flex:"1vw",children:["We should memoize any active ",Object(j.jsx)("i",{children:"GPUComputationRenderers"})," as there is considerable overhead to initialising one. Performance would drop considerably initialising one each frame."]})]})]}),Object(j.jsx)("br",{id:"step3end"}),Object(j.jsx)("br",{}),Object(j.jsxs)("p",{children:["Finally, one extra feature I added at this step was to map (not clamp) all ",Object(j.jsx)(h.a,{children:"$M_{x,y}$"})," to ",Object(j.jsx)(h.a,{children:"$[0,1]$"})," so that the entire range of gradient magnitudes is displayed. To do this, the maximum magnitude in ",Object(j.jsx)(h.a,{children:"$M_{x,y}$"})," is required. Unfortunately, there is no easy way to output a single value from the GPUComputationRenderer, so instead an ",Object(j.jsx)(h.a,{children:"$O(N^2)$"})," CPU-bound pass over ",Object(j.jsx)(h.a,{children:"$M_{x,y}$ "}),"is performed to calculate ",Object(j.jsx)(h.a,{children:"$\\max(M_{x,y})$"}),". This is the only CPU-bound part of the entire process, and can be removed if greater performance is needed. However for visual clarity, and the little effect it has on reasonably sized images, I included this pass."]}),Object(j.jsx)("br",{}),Object(j.jsx)(c.a,{style:{borderTopWidth:"1px",borderTopColor:"#000000",opacity:.5}}),Object(j.jsx)("h1",{id:"nms",className:"raleway-title",children:"Non-maximum suppression"}),Object(j.jsxs)("p",{children:["Non-maximum suppression is a method for thinning the edges produced by the gradient calculation. It pretty much does exactly what it says it does - suppresses (i.e. sets their value to zero) pixels which are not the maximum gradient value along the line perpendicular to the edge at that point. We can find neighbouring points along this line using the argument (angle) we calculated earlier.",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"It is possible that this line does not exactly point in the direction of a neighbouring pixel (i.e. the argument is not a multiple of 45deg). We could simply take the value from the closest neighbouring pixel to this line, but we can do a little better by ",Object(j.jsx)("i",{children:"interpolating"})," the gradient value between the two adjacent pixels to the line.",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"In every case, two of the four interpolation points are on a diagonal from the centre pixel (Note we need to check the magnitudes in both directions along the line, hence 4 points total). Let these points be denoted by ",Object(j.jsx)(h.a,{children:"$a_1,a_2$"}),". We can then choose the second interpolation points ",Object(j.jsx)(h.a,{children:"$b_1,b_2$"})," based on whether the gradient line's horizontal or vertical component is larger in length (absolute value). This is illustrated below with both cases:"]}),Object(j.jsxs)("div",{style:{paddingBottom:"20px",textAlign:"center"},children:[Object(j.jsx)(a.a,{src:l,fallback:"NMS Interpolation Geometry"}),Object(j.jsx)("p",{style:{fontSize:"0.75em"},children:Object(j.jsx)("i",{children:"(Interpolation between pixels) "})})]}),Object(j.jsxs)("p",{children:["We then do a standard linear interpolation between the two magnitudes for both ",Object(j.jsx)(h.a,{children:"$a_1,b_1$"})," and ",Object(j.jsx)(h.a,{children:"$a_2,b_2$"})," using a ratio:"]}),Object(j.jsx)("br",{}),Object(j.jsx)("div",{style:{paddingLeft:"3em",paddingRight:"3em",textAlign:"center"},children:Object(j.jsx)(h.a,{children:"$\\text{lerp} = a_ir + (1-r)b_i$"})}),Object(j.jsx)("br",{}),Object(j.jsxs)("p",{children:[Object(j.jsx)(h.a,{children:"$r$"})," and ",Object(j.jsx)(h.a,{children:"$1-r$"})," are calculated from the larger absolute value of the horizontal ",Object(j.jsx)(h.a,{children:"$\\big(\\cos(\\alpha)\\big)$ "}),"and vertical ",Object(j.jsx)(h.a,{children:"$\\big(\\sin(\\alpha)\\big)$ "})," components as mentioned before and illustrated above. This results in the a final piecewise function which calculates the interpolated gradient value for both directions along the gradient line:"]}),Object(j.jsx)("br",{}),Object(j.jsx)("div",{style:{paddingLeft:"3em",paddingRight:"3em",textAlign:"center",overflow:"hidden"},children:Object(j.jsx)(h.a,{children:"$f(a_i, b_i, \\alpha)=\\begin{cases} M_{a_i} \\cdot {|\\cos(\\alpha)|} + (1-{|\\cos(\\alpha)|}) \\cdot M_{b_i} & \\text{if } \\cos(\\alpha) \\ge \\sin(\\alpha)\\\\  M_{a_i} \\cdot {|\\sin(\\alpha)|} + (1-{|\\sin(\\alpha)|}) \\cdot M_{b_i} & \\text{otherwise}\\end{cases}$"})}),Object(j.jsx)("br",{}),Object(j.jsxs)("p",{children:["Finally, we check the two interpolated values against the magnitude of the centre pixel to decide if the pixel should be eliminated. ",Object(j.jsx)(h.a,{children:"$\\big(\\text{if }M_{\\text{current}} < f(a_1,b_1,\\alpha) \\text{ or }M_{\\text{current}} < f(a_2,b_2,\\alpha)\\big)$ "}),Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"This is all quite simple to implement in a fragment shader which can then be used in the GPUComputationRenderer class mentioned in the previous section."]}),Object(j.jsx)("br",{}),Object(j.jsx)(c.a,{style:{borderTopWidth:"1px",borderTopColor:"#000000",opacity:.5}}),Object(j.jsx)("h1",{id:"threshold",className:"raleway-title",children:"Double thresholding"}),Object(j.jsxs)("p",{children:["With non-maximum suppression completed, we have information about all the local edges in the images as thin lines. We now start taking away irrelevant or obtrusive information in the image.",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"With a process named \"double-thresholding\" we can label edges as strong (ones we wish to keep) and weak (ones which we should probably get rid of). A good example of where this can be useful is in soft edges - think bloom from a light source for example. A specific example is if we wished to find the silhouette of the sun (a circle) from a photo, we would get a well-defined edge from the silhouette, but a bunch of weak, contour like edges from the bloom of the sun. Double thresholding would allow us to mark these as 'weak' and remove them if we wish.",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"We define two thresholds:"]}),Object(j.jsxs)("div",{style:{paddingLeft:"3em",paddingRight:"3em"},children:[Object(j.jsxs)(n.a,{style:{display:"flex",alignItems:"center",paddingBottom:"10px"},children:[Object(j.jsxs)(r.a,{children:[Object(j.jsx)("b",{children:"High "})," \xa0\xa0\xa0"]}),Object(j.jsxs)(r.a,{flex:"1vw",children:["A percentage of the maximum gradient magnitude for which any pixel that exceeds this value is marked as ",Object(j.jsx)("b",{children:'"strong"'})]})]}),Object(j.jsxs)(n.a,{style:{display:"flex",alignItems:"center"},children:[Object(j.jsxs)(r.a,{children:[Object(j.jsx)("b",{children:"Low "})," \xa0\xa0\xa0"]}),Object(j.jsxs)(r.a,{flex:"1vw",children:["A percentage of ",Object(j.jsx)("b",{children:"High"})," for which any pixel that does not exceed high, but exceeds low is marked as ",Object(j.jsx)("b",{children:'"weak"'})]})]})]}),Object(j.jsx)("br",{}),Object(j.jsxs)("p",{children:["This can also be written as an inequality for clarity: ",Object(j.jsx)(h.a,{children:"$0 \\le r_{low}\\big(r_{high}\\max(M)\\big) \\le \\text{weak} \\lt r_{high}\\max(M)\\le \\text{strong}\\le 1,\\ r\\in [0,1]$"}),".",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"Implementing this as a shader pass, we can set any strong pixels to 1, and any weak to a lower opacity value, such as 0.3."]}),Object(j.jsx)("br",{}),Object(j.jsx)(c.a,{style:{borderTopWidth:"1px",borderTopColor:"#000000",opacity:.5}}),Object(j.jsx)("h1",{id:"hysteresis",className:"raleway-title",children:"Edge tracking (via hysteresis)"}),Object(j.jsxs)("p",{children:["Now that the edges of the image are labelled weak and strong, we can start choosing which weak edges we'd like to keep, and which to remove. Hysteresis does this by saying, for each strong edge, mark any neighbouring weak edges as strong as well, then repeat the process for any weak edges changed. In other words, if there is a strong pixel on any part of a continuous edge, we want to mark the entire edge as strong.",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"The hysteresis process lends itself very well to recursion, but this is a problem when programming shaders, as this sort of recursion is not possible on a GPU. Instead, we can approach hysteresis by repeatedly 'growing' strong pixels in the image. The formal name for this process is ",Object(j.jsx)("i",{children:"Morphological dilation"}),", and was introducted to me in ",Object(j.jsx)("a",{href:"https://dahtah.github.io/imager/canny.html",target:"_blank",children:"a blog post by Simon Barthelm\xe9 about canny edge detection in R"}),".",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"We can therefore iteratively grow the strong pixels in the image, outputting new strong pixels wherever there is an overlap with weak pixels, until all desired weak pixels have been changed to strong pixels. This closely resembles a fixed-point iteration. In this implementation, the number of iterations is set by a slider and not determined automatically for two reasons: 1) It is difficult to output when there have been no changes made in a pass of this process as it isn't possible to output a single value from a shader - the same problem as that encountered at the end of ",Object(j.jsx)(s.a,{smooth:!0,to:"#step3end",children:"step 3"}),". And 2) If the number of passes is automatically determined, this might lead to a large performance drop if there are too many passes.",Object(j.jsx)("br",{}),Object(j.jsx)("br",{}),"Finally, I also added a pixel threshold variable for this process which determines how much the strong pixels grow, or in other words how close weak pixels have to be to strong pixels to be converted. This allows for a little better control of the output for this step."]}),Object(j.jsx)("br",{}),Object(j.jsx)(c.a,{style:{borderTopWidth:"1px",borderTopColor:"#000000",opacity:.5}}),Object(j.jsx)("h1",{id:"cleanup",className:"raleway-title",children:"Final clean-up"}),Object(j.jsx)("p",{children:"Finally, any remaining weak edges are removed from the image, and we are left with the finished edge detection image!"})]})}}}]);
//# sourceMappingURL=17.58919ca3.chunk.js.map