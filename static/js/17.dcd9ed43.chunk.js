(this.webpackJsonpapp=this.webpackJsonpapp||[]).push([[17],{103:function(e,t,i){},106:function(e,t,i){"use strict";var n=i(0),a=i(5),r=i(43),s=i(173),o=i(197),c=i(519),l=i(520),d=(i(103),i(7));t.a=function(e){var t=e.title,i=e.githubURL,h=e.projectRoute,b=e.projectLink,m=e.thumb,j=Object(a.h)().pathname;return Object(n.useEffect)((function(){window.scrollTo(0,0)}),[]),Object(d.jsxs)(d.Fragment,{children:[Object(d.jsx)("div",{style:{marginTop:"-3rem",backgroundImage:"url(".concat(m,")"),backgroundPosition:"center",backgroundSize:"cover",backgroundRepeat:"no-repeat",height:"100vh",zIndex:-1}}),Object(d.jsxs)("div",{className:"project-home-wrapper",style:{position:"absolute",width:"100%",top:"101vh",left:"0px",transform:"translate(0, -100%)"},children:[Object(d.jsxs)("header",{className:"home-header",children:[Object(d.jsx)("h1",{id:"title",style:{display:"inline-block"},children:t}),Object(d.jsxs)("span",{style:{padding:"0 1em",display:"inline-block"},children:[void 0!=i?Object(d.jsx)(s.a,{title:"View on Github",placement:"bottom",children:Object(d.jsx)("a",{href:i,target:"_blank",children:Object(d.jsx)(c.a,{className:"title-icon"})})}):null,void 0!=h?Object(d.jsx)(s.a,{title:"View project",placement:"bottom",children:Object(d.jsx)(r.b,{to:j+h,children:Object(d.jsx)(l.a,{className:"title-icon"})})}):null,void 0!=b?Object(d.jsx)(s.a,{title:"View project",placement:"bottom",children:Object(d.jsx)("a",{href:b,target:"_blank",children:Object(d.jsx)(l.a,{className:"title-icon"})})}):null]}),Object(d.jsx)(o.a,{style:{borderTopWidth:"1px",borderTopColor:"#000000",opacity:.5}})]}),Object(d.jsx)("div",{style:{height:"8em"}})]})]})}},108:function(e,t,i){"use strict";var n=i(107),a=i.n(n),r=i(106),s=i(18),o=i(513),c=i(512),l=i(0),d=i(7),h=function(e){var t=e.filter((function(e){var t=e.nodeName;return"title"!==e.id&&("H1"===t||"H2"===t||"H3"===t)})),i=-1,n=t.map((function(e){return i++,{key:"contents_".concat(i),href:"#".concat(e.id),title:e.innerHTML,level:parseInt(e.nodeName.slice(-1))}})),a=[],r=[],s=[a];return n.forEach((function(e){var t=r.findIndex((function(t){return t>=e.level}));-1===t?t=r.push(e.level)-1:r.length=t+1,s[t].push(Object.assign({},e,{children:s[t+1]=[]}))})),a},b=function(e){var t=e.title,i=function(){var e=Object(l.useState)([]),t=Object(s.a)(e,2),i=t[0],n=t[1];return Object(l.useEffect)((function(){var e=Array.from(document.querySelectorAll("h1, h2, h3")),t=h(e);n(t)}),[]),{nestedHeadings:i}}().nestedHeadings,n=function(){var e=Object(l.useState)(0),t=Object(s.a)(e,2),i=t[0],n=t[1];return Object(l.useEffect)((function(){var e=document.getElementById("main-navbar");console.log(e),n(e.offsetHeight)}),[]),{navHeight:i}}().navHeight,a=Object(l.useState)("85vh"),r=Object(s.a)(a,2),b=r[0],m=r[1];return Object(l.useEffect)((function(){var e=document.getElementById("toc-breadcrumb");e&&m("calc(100vh - 6rem - ".concat(e.offsetHeight,"px)"))}),[]),Object(d.jsxs)(d.Fragment,{children:[Object(d.jsx)(o.a,{id:"toc-breadcrumb",style:{paddingBottom:"14px",position:"sticky"},items:[{title:Object(d.jsx)("a",{href:"#home",children:"Portfolio"})},{title:Object(d.jsx)("a",{href:"#projects",children:"Projects"})},{title:"".concat(t)}]}),Object(d.jsx)(c.a,{style:{maxHeight:b,overflow:"auto"},targetOffset:n,onClick:function(e,t){e.preventDefault()},items:i})]})},m=i(131),j=i(257),p=i(258),g=m.a.useBreakpoint;t.a=function(e){var t=e.title,i=e.thumb,n=e.projectLink,s=e.projectRoute,o=e.githubURL,c=e.footer,l=e.children,h=g();return Object(d.jsx)(d.Fragment,{children:Object(d.jsxs)(a.a,{children:[Object(d.jsx)(r.a,{title:t,thumb:i,projectRoute:s,projectLink:n,githubURL:o}),Object(d.jsxs)(j.a,{gutter:0,children:[Object(d.jsx)(p.a,{xs:0,lg:5,children:Object(d.jsx)("div",{className:"project-toc-wrapper",children:Object(d.jsx)(b,{title:t})})}),Object(d.jsxs)(p.a,{xs:24,lg:19,children:[Object(d.jsx)("div",{className:"project-content-wrapper",style:{marginRight:h.lg?"17.5vw":"6vw",marginLeft:h.lg?0:"6vw"},children:l}),Object(d.jsx)("div",{className:"project-footer-wrapper",style:{display:"flex",justifyContent:"center",marginTop:"8vh",marginBottom:"5vh",marginRight:h.lg?"17.5vw":"6vw",marginLeft:h.lg?0:"6vw"},children:c?{footer:c}:"\u274b That's all! Thanks for reading. \u274b"})]})]})]})})}},128:function(e,t,i){"use strict";var n=i(46),a=i(118),r=(i(0),i(508)),s=i(7);t.a=function(e){var t=e.annotation,i=e.fontSize,o=Object(a.a)(e,["annotation","fontSize"]),c=o.paddingBottom?o.paddingBottom:"20px";return Object(s.jsxs)("div",{style:{position:"relative"},children:[Object(s.jsx)(r.a,Object(n.a)({},o)),t?Object(s.jsx)("div",{className:"styled-text",style:{position:"absolute",bottom:0,left:0,backgroundColor:"rgba(21, 25, 31, 0.65)",width:"100%",fontSize:i&&i,textAlign:"center",padding:"10px 5px",paddingBottom:c},children:t}):null]})}},229:function(e,t,i){"use strict";i.r(t);i(0);var n=i(197),a=i(517),r=i(504),s=i.p+"static/media/Evaluating_EP56.71ddb9eb.mp4",o=i.p+"static/media/Rclip_Comparison.1e29e8cb.png",c=i.p+"static/media/216x64xn.d5547d33.svg",l=i.p+"static/media/compare_dqn.db755112.png",d=i.p+"static/media/STACK_DDQN_3.44bb37f1.png",h=i.p+"static/media/STACK_DDQN_3 (EVAL_3600).212306fc.png",b=i.p+"static/media/training_montage.5d359d49.mp4",m=(i(125),i(126)),j=i(148),p=i(108),g=i(128),u=i(7);t.default=function(){return Object(u.jsxs)(p.a,{title:j.a.title,thumb:j.a.thumb,children:[Object(u.jsx)("h1",{id:"overview",className:"raleway-title",children:"Overview"}),Object(u.jsx)("div",{style:{paddingBottom:"20px",textAlign:"center"},children:Object(u.jsx)("video",{style:{objectFit:"cover",width:"100%"},autoPlay:!0,loop:!0,muted:!0,children:Object(u.jsx)("source",{src:s,type:"video/mp4"})})}),Object(u.jsxs)("p",{children:["Reinforcement learning (RL) is a subfield of machine learning that focuses on training agents to make sequential decisions in an environment to maximize a cumulative reward. Unlike supervised learning, where labeled examples are used to directly learn a mapping between inputs and outputs, reinforcement learning involves an agent interacting with an environment, learning from feedback signals (rewards or penalties) to improve its decision-making abilities over time.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"In this post, I provide a short walkthrough on how to train a reinforcement learning agent to tackle OpenAI's CarRacing domain, the results of which can be seen above. OpenAI's Car Racing Environment serves as an excellent benchmark for testing the capabilities of reinforcement learning algorithms. An agent in this environment must train to recognise patterns from pixel data, and learn to control a physically-simulated car according to what it sees. The dimensionality of this domain is somewhat high so we cannot apply brute-force learning methods, and instead must make use of approximation. Neural Networks are an amazing tool for this, and have paved the way for many breakthroughs in the field of RL within the last decade. For example, two you may be familiar with: AlphaGo and OpenAI Five."]}),Object(u.jsx)("br",{}),Object(u.jsx)(n.a,{style:{borderTopWidth:"1px",borderTopColor:"#000000",opacity:.5}}),Object(u.jsx)("h1",{id:"rl-into",className:"raleway-title",children:"A Brief intro to RL"}),Object(u.jsxs)("p",{children:["The RL problem involves an agent interacting with an environment, where the agent takes actions based on its observations, receives feedback in the form of rewards, and learns to improve its decision-making abilities as it trains. The problem can be formalized as a Markov Decision Process (MDP), which consists of a set of states, actions, transition probabilities, and rewards. The goal of the RL agent is to learn an optimal policy - a mapping from states to actions - that maximizes the cumulative reward it receives over time in the environment.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"We encounter a different set of machine learning problems with RL. For instance, our reward function can be sparse, making optimisation tricky. Additionally, an agent has to consider a lot of unknowns; it does not ncessarily know what the result of taking a specific action may be - it must learn from experience. This introduces a trade-off between exploration: trying out new actions to learn more about the environment, and exploitation: choosing actions that are known to yield higher rewards."]}),Object(u.jsx)("br",{}),Object(u.jsx)(n.a,{style:{borderTopWidth:"1px",borderTopColor:"#000000",opacity:.5}}),Object(u.jsx)("h1",{id:"dqn",className:"raleway-title",children:"Deep Q Learning (DQN)"}),Object(u.jsxs)("p",{children:["DQN is a friendly starting point for training an RL agent due to its simplicity and versatile training process. DQN involves learning a 'Q-value' for each state-action pair which is an estimate of the expected reward an agent would receive for taking a particular action in a particular state. An agent can then choose sensible actions by greedily selecting actions which lead to the highest expected reward.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"Two observations we can make from this explanation are 1: since we have state-action pairs, we must be working with a discrete action space. And 2: the size of the state-action pair space is likely very large, or infinite for more challenging problems. This is where the 'deep' part of Deep-Q comes from. We cannot possibly generate an optimal set of Q-values for an infinitely large space, so we make use of deep learning to approximate this Q-value function for all states. DQN is infact based off of an algorithm which indeed tabulates all possible Q-values, named Q-learning, but this is obviously not applicable to this more complex domain.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"An important consideration in the CarRacing domain is how to deal with the large observation space we have. The observation of pixel data is a ",Object(u.jsx)(m.a,{children:"$96\\times96$"})," matrix (at a minimum, 9216 values!), which would make training slow for a regular dense network. The obvious choice instead is to use a convolutional neural network (CNN) to reduce the input dimensions and to learn spatial-features (e.g. distinguishing track from off-road) before passing information to a dense network."]}),Object(u.jsx)("br",{}),Object(u.jsx)("h1",{id:"ddqn",className:"raleway-title",children:"Double Deep Q (DDQN)"}),Object(u.jsxs)("p",{children:["DQN, as introduced above, is a method which is able to learn an optimal policy through approximation of Q-values (from ideas in tabular Q-learning). Q-values describe the expected return from taking a certain action in a certain state. This allows an agent to make decisions in its current state without having to look forward in time, which is often not possible. An agent can therefore greedily select actions with the highest Q-values in order to follow it's current best policy. A deep model is used to generate approximate Q-values from a state-observation. Policies are refined by reducing loss calculated differences in current vs. expected q-value based on the reward received from many state-transitions (experiences).",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"DDQN expands on these ideas slightly by tackling a potent issue in DQN: overestimation bias. Since an agent greedily follows its best policy to evaluate expected return, it is likely to make an overestimate - it is only concerned with the best possible outcomes in the future. In actuality, not all outcomes will be 'optimal' and so the value is higher than expected. This issue is additionally worsened when the frequency of estimations is increased.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"DDQN reduces overestimation bias by maintaining two networks; an online 'training' network for action selection, and an offline 'target' network for q-value estimation. The target network is updated to match the training network periodically, thus stays reasonably stable for q-value estimation and reduces overestimation. This changes the loss calculation under MSE for non-terminal ",Object(u.jsx)(m.a,{children:"$s'$"})," from",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsx)("div",{style:{paddingLeft:"3em",paddingRight:"3em",textAlign:"center"},children:Object(u.jsx)(m.a,{children:"$L(s,a,s',r)=\\bigg(\\big(r+\\gamma\\max_{a'}Q(s',a')\\big)-Q(s,a)\\bigg)^2$"})}),Object(u.jsx)("br",{}),"to",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsx)("div",{style:{paddingLeft:"3em",paddingRight:"3em",textAlign:"center"},children:Object(u.jsx)(m.a,{children:"$L(s,a,s',r)=\\bigg(\\underbrace{\\big(r+\\gamma\\max_{a'}Q_{\\text{target}}(s',a')\\big)}_{\\text{new q-value estimate } \\hat q}-Q_{\\text{train}}(s,a)\\bigg)^2$"})}),"(for terminal ",Object(u.jsx)(m.a,{children:"$s'$"}),", DDQN still uses ",Object(u.jsx)(m.a,{children:"$\\hat q = r$"}),")"]}),Object(u.jsx)("br",{}),Object(u.jsx)(n.a,{style:{borderTopWidth:"1px",borderTopColor:"#000000",opacity:.5}}),Object(u.jsx)("h1",{id:"preprocessing",className:"raleway-title",children:"Preprocessing"}),Object(u.jsxs)("p",{children:["The default state space provided by the environment contains a lot of information which is effectively redundant to an agent. In fact, a lot of this information makes training the agent ",Object(u.jsx)("i",{children:"significantly harder"})," as it introduces a lot of noise into the agent's observation while providing negligible meaningful content."]}),Object(u.jsx)("div",{style:{margin:"0 auto",paddingBottom:"20px",width:"100%"},children:Object(u.jsx)(g.a,{width:"100%",src:"data:image/png;base64,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"})}),Object(u.jsxs)("p",{children:["The exact steps are as follows:",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsx)("b",{children:Object(u.jsx)("u",{children:"1. Bottom UI Bar Removed."})}),Object(u.jsx)("br",{}),"Mostly is noise. Contains visualisations of the agents score, driving speed and turning speeds, but information is hard to extract from this due to pooling layers.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsx)("b",{children:Object(u.jsx)("u",{children:"2. Billinear downscaling replaced by nearest-neighbour downscaling."})}),Object(u.jsx)("br",{}),"Billinear downscaling introduces unnecessary noise (blurring) along the road boundary and around the car's position. It also causes the number of unique colour values in the observation to increase which may make mapping features more difficult.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsx)("b",{children:Object(u.jsx)("u",{children:"3. Green grass pixels clamped to white."})}),Object(u.jsx)("br",{}),"Further removal of unecessary information; the agent only needs to know where the grass boundary is. It generally should not be going into the grass as this has no positive reward. Clamping to white is sufficient, and makes the boundary more distinct from other features in the observation.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsx)("b",{children:Object(u.jsxs)("u",{children:["4. RGB ",Object(u.jsx)(m.a,{children:"$\\to$"})," Grayscale conversion."]})}),Object(u.jsx)("br",{}),"Cuts the observation size down to a third without losing important information; all key features (road, grass, checkpoints, and car) are still unique colours in mono.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsx)("b",{children:Object(u.jsxs)("u",{children:["5. Grayscale values ",Object(u.jsx)(m.a,{children:"$\\in[0, 1]$"})," normalised to the range ",Object(u.jsx)(m.a,{children:"$[-1, 1]$"}),"."]})}),Object(u.jsx)("br",{}),"A generally useful trick for training neural networks. Helps reduce vanishing and exploding gradients, and smooths the optimisation landscape.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"Additionally, some undesired behaviour of the environment was also removed, such as the view of the track zooming in within the first few frames; this would make the agent's observation fundamentally different within the first few frames and subsequently harder to learn from or generalise the rest of its racing strategy to."]}),Object(u.jsx)("br",{}),Object(u.jsx)(n.a,{style:{borderTopWidth:"1px",borderTopColor:"#000000",opacity:.5}}),Object(u.jsx)("h1",{id:"tweaks",className:"raleway-title",children:"Algorithm Tweaks"}),Object(u.jsx)("h2",{id:"acsp",className:"raleway-title",children:"Action Space"}),Object(u.jsxs)("p",{children:["By default, the environment operates under continuous actions. This is suitable for PPO, however DDQN operates only on discrete action spaces. As such, the action space is reduced to a set of 5 primary actions for DDQN models which capture the main extents of which the car can be controlled: full left & right turning, full acceleration, and soft braking (full braking only is too harsh for smooth control).",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"Additionally, training can be quite slow from the offset as the car starts from a standstill. The only useful action an agent can take in this state is acceleration; all others keep the car stuck in the standstill. Therefore a parameter can be given to bias acceleration actions when choosing a random action. For instance, the probability distribution for the reduced action space under an acceleration bias factor 4 would be [0.125, 0.125, 0.5, 0.125, 0.125]. This helps speed up the start of training by increasing the change of generating useful experiences from the offset of training."]}),Object(u.jsx)("br",{}),Object(u.jsx)("h2",{id:"frame-stack",className:"raleway-title",children:"Frame Stacking"}),Object(u.jsxs)("p",{children:["Frame skipping refers to allowing the agent to only act every ",Object(u.jsx)(m.a,{children:"$N$"})," frames, and having it repeat the same action in the between-frames. On its own, this allows for an agent to train faster as it will only train every ",Object(u.jsx)(m.a,{children:"$N$"})," frames; training on frame data is expensive and it is more important that diverse experiences are gathered rather than large volumes for effective learning.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"Frame stacking refers to aggregating multiple sequentially observed frames into a single observation. The creators of the technique saw success with it when playing Atari games, to which this domain is similar. Use of the aforementioned between-frames from frame skipping is an easy way of creating such a frame stack, i.e. combining a number of 2D observations (frames) into a single 3D observation. This increases the observation space substantially, but is manageable to train on using convolution and pooling. Frame stacking is very beneficial however due to the fact that it adds previously absent temporal information into the observation. This allows an agent to gauge information about its speed, or turning speed, which is not possible without. This is ",Object(u.jsx)("i",{children:"extremely"})," important for making tight turns and preparing ahead, as prepration for a turn really begins some time before hitting the apex. Unreliable preparation for turns is a major problem without frame stacking, and perhaps the single most important feature for increased general performance based on experimental results."]}),Object(u.jsx)("br",{}),Object(u.jsx)("h2",{id:"batch-prio",className:"raleway-title",children:"Priority Batches"}),Object(u.jsxs)("p",{children:["By default, experience replay batches are uniformly sampled from the agent's memory buffer. This ensures the agent learns from all memories currently in the buffer given enough updates. This approach is generally fine, but when working on a reasonably complex environment (especially with limited computing power), learning like this can be quite slow. An agent can accumulate a fair amount of experience - especially with a large replay buffer - but not actually learn from the ",Object(u.jsx)("i",{children:"useful"})," experience it has gathered (e.g. major control mistakes), as batches get diluted with rudimentary errors from early training.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"Prioritised Experience Replay (PER) has been proposed to ensure agents learn from the most salient errors in their gathered experience. Samples are importance sampled using the TD-error in the DDQN update step as the sampling probability distribution function (pdf).",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"Taking ideas from the above, a simplified pdf can be defined under the assumption that, generally speaking, more recently gathered experiences are more useful to learn from as it is more likely the agent has a decent level of control over the car in these experiences as training progresses. This can potentially help the agent to learn a more optimal strategy but its main purpose is for reducing convergence times. A simple linear function over the indices of items in the replay buffer works well (derived from Gauss' sum to ",Object(u.jsx)(m.a,{children:"$n$"})," numbers):",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsxs)("div",{style:{paddingLeft:"3em",paddingRight:"3em",textAlign:"center"},children:[Object(u.jsx)(m.a,{children:"$\\displaystyle pdf(i)=\\frac{2i}{N(N+1)},$"}),"\xa0\xa0\xa0",Object(u.jsx)(m.a,{children:"$N=len(\\text{Replay Buffer}),$"}),"\xa0\xa0\xa0",Object(u.jsx)(m.a,{children:"$i\\in [1,\\ N+1]$"})]})]}),Object(u.jsx)("br",{}),Object(u.jsx)("h2",{id:"reward-clip",className:"raleway-title",children:"Reward Clipping"}),Object(u.jsxs)("p",{children:["Agents have a tendency to act greedily to accumulate reward, and this is especially noticeable for short-term when the car is moving quickly. High rewards can be accumulated in a single frame when the car crosses multiple checkpoints at once due to high speeds. On subsequent iterations, these translate into high short-term rewards (only minorly ",Object(u.jsx)(m.a,{children:"$\\gamma$"}),"-discounted) and encourage reckless driving tactics, seen mostly as extremely high speeds on straightaways. Cornering is near impossible at these speeds and is a major case of failure. Subsequently, rewards can be ",Object(u.jsx)("i",{children:"clipped"})," - 'truncated' so to speak - so no more than a little reward can be accumulated each frame. This implicitly discourages greedy behaviour. In this implementation, cumulative rewards from frame-skips are clamped to the range ",Object(u.jsx)(m.a,{children:"$[-10, \\ 1]$"}),". The graph below shows experimentally that reward clipping can help training converge more quickly - almost twice as quickly in this example (~400 episode convergence with; ~800 episode convergence without)."]}),Object(u.jsx)("div",{style:{margin:"0 auto",paddingBottom:"20px",width:"100%",maxWidth:"574px"},children:Object(u.jsx)(g.a,{src:o})}),Object(u.jsx)("br",{}),Object(u.jsx)("h2",{id:"other-tweak",className:"raleway-title",children:"Other Tweaks"}),Object(u.jsxs)("p",{children:[Object(u.jsx)("b",{children:Object(u.jsx)("u",{children:"Regularization for networks (L2)"})}),Object(u.jsx)("br",{}),"A general machine learning technique. Aims to help prevent overfitting and promote generalisation. In this case, deterring a network from relying on any particular node too much may prevent the agent from learning unusual quasi-effective strategies.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsx)("b",{children:Object(u.jsx)("u",{children:"Exponentially decaying learning rate"})}),Object(u.jsx)("br",{}),"Another general machine learning technique. Helps fine-tune a model after it has done significant learning in the beginning of training - when major changes to policy are likely no longer needed.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsx)("b",{children:Object(u.jsx)("u",{children:"Early Stopping"})}),Object(u.jsx)("br",{}),"Stopping episodes early when an agent is performing badly. This helps train the agent faster (episodes are generally shorter) and to not dilute the agent's gathered experience with states it should not have found itself in to begin with. This involves stopping episodes when cumulative reward drops below a certain threshold (-10) or when the agent receives negative reward n frames in a row (hyperparameter)."]}),Object(u.jsx)("br",{}),Object(u.jsx)(n.a,{style:{borderTopWidth:"1px",borderTopColor:"#000000",opacity:.5}}),Object(u.jsx)("h1",{id:"architecture",className:"raleway-title",children:"Final Network Architecture"}),Object(u.jsx)("p",{children:"Below is a visualisation of the final CNN architecture used as well as its corresponding Tensorflow summary."}),Object(u.jsx)("div",{style:{margin:"0 auto",paddingBottom:"20px",width:"100%",maxWidth:"1200px",backgroundColor:"white"},children:Object(u.jsx)("img",{src:c})}),Object(u.jsx)("br",{}),Object(u.jsx)("div",{className:"code-snippet",style:{width:"100%"},children:Object(u.jsx)(a.a,{language:"python",showLineNumbers:!1,style:r.a,startingLineNumber:0,children:'\nModel: "sequential"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #\n=================================================================\n Convolutional_1 (Conv2D)    (None, 30, 30, 6)         300\n\n max_pooling2d (MaxPooling2D  (None, 15, 15, 6)        0\n )\n\n Convolutional_2 (Conv2D)    (None, 12, 12, 12)        1164\n\n max_pooling2d_1 (MaxPooling  (None, 6, 6, 12)         0\n 2D)\n\n flatten (Flatten)           (None, 432)               0\n\n Dense1 (Dense)              (None, 216)               93528\n\n Dense2 (Dense)              (None, 64)                13888\n\n dense (Dense)               (None, 5)                 325\n\n=================================================================\nTotal params: 109,205\nTrainable params: 109,205\nNon-trainable params: 0\n_________________________________________________________________\n'})}),Object(u.jsx)("br",{}),Object(u.jsx)(n.a,{style:{borderTopWidth:"1px",borderTopColor:"#000000",opacity:.5}}),Object(u.jsx)("h1",{id:"results",className:"raleway-title",children:"Results"}),Object(u.jsx)("h2",{id:"baselines",className:"raleway-title",children:"Baselines"}),Object(u.jsxs)("p",{children:["Firstly, some baselines:",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsx)("b",{children:Object(u.jsx)("u",{children:"Solved"})}),Object(u.jsx)("br",{}),"Score ",Object(u.jsx)(m.a,{children:"$\\gt$"})," 900 for single episodes. The environment is considered fully solved when an average score of 900 is achieved across 100 consecutive episodes.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsx)("b",{children:Object(u.jsx)("u",{children:"Zero-performance"})}),Object(u.jsx)("br",{}),"Random Agent. Achieves an average score of ",Object(u.jsx)("b",{children:"-10"}),".",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsx)("b",{children:Object(u.jsx)("u",{children:"Low-performance"})}),Object(u.jsx)("br",{}),"Forward Only Agent. Achieves an average score of ",Object(u.jsx)("b",{children:"80"}),".",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsx)("b",{children:Object(u.jsx)("u",{children:"Human"})}),Object(u.jsx)("br",{}),"Anywhere from ",Object(u.jsx)("b",{children:"low 100s"})," all the way up to full solves depending on experience."]}),Object(u.jsx)("br",{}),Object(u.jsx)("h2",{id:"graph",className:"raleway-title",children:"Graphs and Discussion"}),Object(u.jsxs)("p",{children:["Overall, through use of DDQN, we can see promising and performative results. All agents exhibited clear attempts to stay on track and drive at high speeds when possible. On the best trained model (STACK_DDQN_3), the agent is close to full episode solves approximately 50% of the time. Additionally, the agent consistently beat both baseline measures, even in episodes with relatively poor performance compared to the max/mean. At it's best performance, it likely drives better than a human expert.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsx)("div",{style:{margin:"0 auto",paddingBottom:"20px",width:"100%",maxWidth:"817px"},children:Object(u.jsx)(g.a,{src:l})}),Object(u.jsx)("br",{}),"We can observe some interesting emergent behaviour from agents, including agents getting back on track after failing (recovery), corner cutting without skipping checkpoints, and tight racing lines (especially around the tightest of corners).",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"Model evaluations indicate that mean, as well as maximum performance can be significantly increased by introducing temporal data into models via framestacking. Without framestacking, agents are not able to fully solve any episodes, and are prone to greedy and reckless driving behaviours. Priority batch sampling an agent's experience during replay significantly reduced total convergence time. A comparison of STACK_DDQN_P_1 and STACK_DDQN_3 indicates that priority batching can reduce convergence times by approximately a half. The rationale for this improvement is that latter agent experience is likely to be closer to good racing policy, and is therefore good to learn from when fine-tuning.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"Moreover, a comparison of DDQN_P_1 and DDQN_P_2 indicates that reward clipping also helped reduce convergence times and overall training consistency. Reward clipping helped reduce overconfident driving, which was especially prominent in agents like the aforementioned which didn't train on temporally-correlated information (no framestacking).",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"One of the most potent issues with trained agents is inconsistency: the standard deviation about the mean score is relatively high for all agents. This is partly due to the fact that the agents are punished harshly for mistakes (via early episode termination) but also due to the fact that sometimes an agent isn't able to perform a simple action which it is able to perform in a seemingly similar scenario. This is most often a failed early turn. This could be due to three reasons: 1) some level of forgetting - an agent dropping an older learned policy in favour of a new one based on new experience. 2) somewhat incoherent models of the track in agents' convolutional layer feature maps. 3) agents sometimes being forced to make suboptimal moves due to turning being somewhat heavy-handed in the discrete action space. Such actions have to be repeated during frame skips also.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"Below is a detailed training graph for the best trained model (STACK_DDQN_3):",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsx)("div",{style:{margin:"0 auto",paddingBottom:"20px",width:"100%",maxWidth:"817px"},children:Object(u.jsx)(g.a,{src:d})}),Object(u.jsx)("br",{}),"And a histogram of evaluation results:",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),Object(u.jsx)("div",{style:{margin:"0 auto",paddingBottom:"20px",width:"100%",maxWidth:"817px"},children:Object(u.jsx)(g.a,{src:h})})]}),Object(u.jsx)("br",{}),Object(u.jsx)("h1",{id:"training",className:"raleway-title",children:"Training Montage"}),Object(u.jsx)("div",{style:{paddingBottom:"20px",textAlign:"center"},children:Object(u.jsx)("video",{style:{objectFit:"cover",width:"100%"},autoPlay:!0,loop:!0,muted:!0,children:Object(u.jsx)("source",{src:b,type:"video/mp4"})})}),Object(u.jsx)("br",{}),Object(u.jsx)("h1",{id:"improvements",className:"raleway-title",children:"Improvements"}),Object(u.jsxs)("p",{children:["First and foremost, it would be beneficial to allow the stacked DDQN models shown to train for more iterations, ideally into the order of millions of training iterations. Current stacked models still show shallow upwards trends when training was halted (especially STACK_DDQN_P_1), so it is possible they would be able to solve the environment fully given enough iterations.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"It would be ideal to experiment further with different training approaches such as Dueling DQN. Dueling DQN could alleviate some of the performance inconsistency observed in late-stage training by assisting the agent in having a more coherent model of its environment due to both training and target networks sharing the same convolutional layers.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"A different training avenue is under a continuous action space, using PPO. PPO has multiple benefits. PPO trains on continuous action spaces, which would allow for better control in difficult scenarios, and would reduce the heavy-handedness of turning overall. It is able to scale up well on hardware (multiple environments can be run in parallel and save experiences to a shared buffer) which would be useful to cut down on the long training times seen with DDQN. PPO using Actor-Critic based updates would hopefully reduce the amount of hyperparameter tweaking which has to be done - this was a major time-sink during experimentation.",Object(u.jsx)("br",{}),Object(u.jsx)("br",{}),"Moving away from technical reinforcement learning improvements, a better baseline which could be implemented is a PID controller - a handcrafted technique designed for autonomous control. This technique finds use in automotive applications such as cruise control and RC racing. It would provide a competitive baseline between the forward-only and human expert baselines."]})]})}}}]);
//# sourceMappingURL=17.dcd9ed43.chunk.js.map