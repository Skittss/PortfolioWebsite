{"version":3,"sources":["projects/homeTemplate.jsx","projects/tableofcontents.jsx","projects/projectPage.jsx","projects/annotatedImage.jsx","content/projects/CarRacing/Evaluating_EP56.mp4","content/projects/CarRacing/graphs/Rclip_Comparison.png","content/projects/CarRacing/graphs/216x64xn.svg","content/projects/CarRacing/graphs/compare_dqn.png","content/projects/CarRacing/graphs/STACK_DDQN_3.png","content/projects/CarRacing/graphs/STACK_DDQN_3 (EVAL_3600).png","content/projects/CarRacing/training_montage.mp4","projects/CarRacing/home.jsx","content/projects/CarRacing/preprocessing/preprocessing_steps.png","content/projects/CarRacing/code_snippets/architecture.js"],"names":["HomeTemplate","title","githubURL","projectRoute","projectLink","thumb","loc","useLocation","pathname","useEffect","window","scrollTo","style","marginTop","backgroundImage","backgroundPosition","backgroundSize","backgroundRepeat","height","zIndex","className","position","width","top","left","transform","id","display","padding","undefined","placement","href","target","to","borderTopWidth","borderTopColor","opacity","getNestedHeadings","headingElements","headingAttention","filter","e","l","nodeName","ki","headingLst","map","key","innerHTML","level","parseInt","slice","result","indices","levels","forEach","o","index","findIndex","push","length","Object","assign","children","TableOfContents","nestedHeadings","useState","setNestedHeadings","Array","from","document","querySelectorAll","newNestedHeadings","useHeadingsData","navHeight","setNavHeight","navbar","getElementById","console","log","offsetHeight","useNavHeight","anchorHeight","setAnchorHeight","breadcrumbRef","paddingBottom","items","maxHeight","overflow","targetOffset","onClick","preventDefault","useBreakpoint","Grid","ProjectPage","footer","screens","gutter","xs","lg","marginRight","marginLeft","justifyContent","marginBottom","AnnotatedImage","annotation","fontSize","props","bottom","backgroundColor","textAlign","Home","Meta","objectFit","autoPlay","loop","muted","src","racing_example","type","paddingLeft","paddingRight","margin","maxWidth","rclip_compare","architecture","language","showLineNumbers","dracula","startingLineNumber","compare_dqn","final_dqn","final_dqn_hist","training_montage"],"mappings":"mHAAA,8EAgEeA,IAzDM,SAAC,GAA0D,IAAzDC,EAAwD,EAAxDA,MAAOC,EAAiD,EAAjDA,UAAWC,EAAsC,EAAtCA,aAAcC,EAAwB,EAAxBA,YAAaC,EAAW,EAAXA,MAC1DC,EAAMC,cAAcC,SAM1B,OAJAC,qBAAU,WACNC,OAAOC,SAAS,EAAG,KAClB,IAGD,qCACA,qBAAKC,MAAO,CACRC,UAAW,QACXC,gBAAgB,OAAD,OAAST,EAAT,KACfU,mBAAoB,SACpBC,eAAgB,QAChBC,iBAAkB,YAClBC,OAAQ,QACRC,QAAS,KAEb,sBAAKC,UAAU,uBAAuBR,MAAO,CAACS,SAAU,WAAYC,MAAO,OAAQC,IAAK,QAASC,KAAM,MAAOC,UAAW,uBAAzH,UACI,yBAAQL,UAAU,cAAlB,UACI,oBAAIM,GAAG,QAAQd,MAAO,CAACe,QAAS,gBAAhC,SAAkD1B,IAClD,uBAAMW,MAAO,CAACgB,QAAS,QAASD,QAAS,gBAAzC,eACkBE,GAAb3B,EACG,cAAC,IAAD,CAASD,MAAM,iBAAiB6B,UAAU,SAA1C,SACI,mBAAGC,KAAM7B,EAAW8B,OAAO,SAA3B,SACI,cAAC,IAAD,CAAgBZ,UAAU,mBAGlC,UAEaS,GAAhB1B,EACG,cAAC,IAAD,CAASF,MAAM,eAAe6B,UAAU,SAAxC,SACI,cAAC,IAAD,CAAMG,GAAI3B,EAAMH,EAAhB,SACI,cAAC,IAAD,CAAciB,UAAU,mBAGhC,UAEYS,GAAfzB,EACG,cAAC,IAAD,CAASH,MAAM,eAAe6B,UAAU,SAAxC,SACI,mBAAGC,KAAM3B,EAAa4B,OAAO,SAA7B,SACI,cAAC,IAAD,CAAcZ,UAAU,mBAGhC,QAGR,cAAC,IAAD,CAASR,MAAO,CAACsB,eAAgB,MAAOC,eAAgB,UAAWC,QAAS,SAEhF,qBAAKxB,MAAO,CAACM,OAAQ,iB,wGClC3BmB,EAAoB,SAACC,GAC1B,IAAIC,EAAmBD,EAAgBE,QAAO,SAAAC,GAC7C,IAAIC,EAAID,EAAEE,SACV,MAAgB,UAATF,EAAEf,KAAyB,OAANgB,GAAoB,OAANA,GAAoB,OAANA,MAGrDE,GAAM,EACNC,EAAaN,EAAiBO,KAAI,SAAAL,GAErC,OADAG,IACQ,CAAEG,IAAI,YAAD,OAAcH,GAAMb,KAAK,IAAD,OAAMU,EAAEf,IAAMzB,MAAOwC,EAAEO,UAAWC,MAAOC,SAAST,EAAEE,SAASQ,OAAO,QAEtGC,EAAS,GACZC,EAAU,GACVC,EAAS,CAACF,GAaX,OAXAP,EAAWU,SAAQ,SAAAC,GAClB,IAAIC,EAAQJ,EAAQK,WAAU,SAAAT,GAAK,OAAIA,GAASO,EAAEP,UACnC,IAAXQ,EACHA,EAAQJ,EAAQM,KAAKH,EAAEP,OAAS,EAEhCI,EAAQO,OAASH,EAAQ,EAE1BH,EAAOG,GAAOE,KAAKE,OAAOC,OAAO,GAAIN,EAAG,CAAEO,SAAUT,EAAOG,EAAQ,GAAK,SAIlEL,GAyDOY,EAzCS,SAAC,GAAe,IAAb/D,EAAY,EAAZA,MAClBgE,EA5De,WAAO,IAAD,EACeC,mBAAS,IADxB,mBACtBD,EADsB,KACNE,EADM,KAc7B,OAXA1D,qBAAU,WACT,IAAM6B,EAAkB8B,MAAMC,KAC7BC,SAASC,iBAAiB,eAIrBC,EAAoBnC,EAAkBC,GAE5C6B,EAAkBK,KAChB,IAEI,CAAEP,kBA8CkBQ,GAAnBR,eACAS,EAfY,WAAO,IAAD,EACQR,mBAAS,GADjB,mBACnBQ,EADmB,KACRC,EADQ,KAU1B,OAPAlE,qBAAU,WACT,IAAMmE,EAASN,SAASO,eAAe,eACvCC,QAAQC,IAAIH,GAEZD,EAAaC,EAAOI,gBAClB,IAEI,CAAEN,aAKaO,GAAdP,UAF8B,EAIER,mBAAS,QAJX,mBAI/BgB,EAJ+B,KAIjBC,EAJiB,KAatC,OAPA1E,qBAAU,WACT,IAAM2E,EAAgBd,SAASO,eAAe,kBAC1CO,GACHD,EAAgB,uBAAD,OAAwBC,EAAcJ,aAAtC,UAEd,IAGF,qCACC,cAAC,IAAD,CAAYtD,GAAG,iBAAiBd,MAAO,CAACyE,cAAe,OAAQhE,SAAU,UACxEiE,MAAO,CACN,CACCrF,MAAO,mBAAG8B,KAAK,QAAR,wBAER,CACC9B,MAAO,mBAAG8B,KAAK,YAAR,uBAER,CACC9B,MAAM,GAAD,OAAKA,OAIb,cAAC,IAAD,CACCW,MAAO,CAAC2E,UAAWL,EAAcM,SAAU,QAC3CC,aAAcf,EACdgB,QAAS,SAACjD,EAAGC,GAEZD,EAAEkD,kBAEHL,MAAOrB,Q,2BC/FH2B,EAAkBC,IAAlBD,cAuCOE,IArCK,SAAC,GAA4E,IAA3E7F,EAA0E,EAA1EA,MAAOI,EAAmE,EAAnEA,MAAOD,EAA4D,EAA5DA,YAAaD,EAA+C,EAA/CA,aAAcD,EAAiC,EAAjCA,UAAW6F,EAAsB,EAAtBA,OAAQhC,EAAc,EAAdA,SACxEiC,EAAUJ,IAEhB,OAEI,mCACA,eAAC,IAAD,WACI,cAAC,IAAD,CACI3F,MAAOA,EACPI,MAAOA,EACPF,aAAcA,EACdC,YAAaA,EACbF,UAAWA,IACf,eAAC,IAAD,CAAK+F,OAAQ,EAAb,UACA,cAAC,IAAD,CAAKC,GAAI,EAAGC,GAAI,EAAhB,SACA,qBAAK/E,UAAU,sBAAf,SACI,cAAC,EAAD,CAAiBnB,MAAOA,QAG5B,eAAC,IAAD,CAAKiG,GAAI,GAAIC,GAAI,GAAjB,UACA,qBAAK/E,UAAU,0BAA0BR,MAAO,CAACwF,YAAaJ,EAAQG,GAAI,SAAW,MAAOE,WAAYL,EAAQG,GAAI,EAAI,OAAxH,SACKpC,IAEL,qBAAK3C,UAAU,yBAAyBR,MAAO,CAC3Ce,QAAS,OAAQ2E,eAAgB,SACjCzF,UAAW,MAAO0F,aAAc,MAChCH,YAAaJ,EAAQG,GAAI,SAAW,MAAOE,WAAYL,EAAQG,GAAI,EAAI,OAH3E,SAKKJ,EAAS,CAACA,UAAU,+D,6ECLtBS,IA1BQ,SAAC,GAAsC,IAArCC,EAAoC,EAApCA,WAAYC,EAAwB,EAAxBA,SAAaC,EAAW,yCAEnDtB,EAAgBsB,EAAMtB,cAAgBsB,EAAMtB,cAAgB,OAElE,OACI,sBAAKzE,MAAO,CAACS,SAAU,YAAvB,UACI,cAAC,IAAD,eAAWsF,IACVF,EACG,qBAAKrF,UAAU,cAAcR,MAAO,CAChCS,SAAU,WACVuF,OAAQ,EACRpF,KAAM,EACNqF,gBAAiB,yBACjBvF,MAAO,OACPoF,SAAUA,GAAYA,EACtBI,UAAW,SACXlF,QAAS,WACTyD,cAAeA,GATnB,SAWKoB,IAEL,U,4ECxBD,MAA0B,4CCA1B,MAA0B,6CCA1B,MAA0B,qCCA1B,MAA0B,wCCA1B,MAA0B,yCCA1B,MAA0B,qDCA1B,MAA0B,6C,oDC2b1BM,UAjaF,WACT,OAEI,eAAC,IAAD,CAAa9G,MAAO+G,IAAK/G,MAAOI,MAAO2G,IAAK3G,MAA5C,UACI,oBAAIqB,GAAG,WAAWN,UAAU,gBAA5B,sBAGA,qBAAKR,MAAO,CAACyE,cAAe,OAAQyB,UAAW,UAA/C,SACI,uBAAOlG,MAAO,CAACqG,UAAW,QAAS3F,MAAO,QAAU4F,UAAQ,EAACC,MAAI,EAACC,OAAK,EAAvE,SACI,wBAAQC,IAAKC,EAAgBC,KAAK,kBAG1C,sfAMI,uBAAM,uBANV,syBAaA,uBACA,cAAC,IAAD,CAAS3G,MAAO,CAACsB,eAAgB,MAAOC,eAAgB,UAAWC,QAAS,MAC5E,oBAAIV,GAAG,UAAUN,UAAU,gBAA3B,iCAGA,ukBAMI,uBAAM,uBANV,ufAaA,uBACA,cAAC,IAAD,CAASR,MAAO,CAACsB,eAAgB,MAAOC,eAAgB,UAAWC,QAAS,MAC5E,oBAAIV,GAAG,MAAMN,UAAU,gBAAvB,mCAGA,ybAII,uBAAM,uBAJV,woBASI,uBAAM,uBATV,kJAWmC,cAAC,IAAD,4BAXnC,uUAeA,uBACA,oBAAIM,GAAG,OAAON,UAAU,gBAAxB,kCAGA,wvBASI,uBAAM,uBATV,scAcI,uBAAM,uBAdV,mYAkByF,cAAC,IAAD,mBAlBzF,QAmBI,uBAAM,uBACN,qBAAKR,MAAO,CAAC4G,YAAa,MAAOC,aAAc,MAAOX,UAAW,UAAjE,SACI,cAAC,IAAD,6FAIJ,uBAzBJ,KA2BI,uBAAM,uBACN,qBAAKlG,MAAO,CAAC4G,YAAa,MAAOC,aAAc,MAAOX,UAAW,UAAjE,SACI,cAAC,IAAD,oLA7BR,iBAiCkB,cAAC,IAAD,mBAjClB,qBAiC2D,cAAC,IAAD,4BAjC3D,OAmCA,uBACA,cAAC,IAAD,CAASlG,MAAO,CAACsB,eAAgB,MAAOC,eAAgB,UAAWC,QAAS,MAC5E,oBAAIV,GAAG,gBAAgBN,UAAU,gBAAjC,2BAGA,2NAEuF,qDAFvF,kHAKA,qBAAKR,MAAO,CAAC8G,OAAQ,SAAUrC,cAAe,OAAQ/D,MAAO,QAA7D,SACI,cAAC,IAAD,CAAgBA,MAAM,OAAO+F,IC3I9B,yjJD6IH,gEAEI,uBAAM,uBACN,4BAAG,4DACH,uBAJJ,sKAOI,uBAAM,uBACN,4BAAG,sGACH,uBATJ,wPAaI,uBAAM,uBACN,4BAAG,0EACH,uBAfJ,qSAmBI,uBAAM,uBACN,4BAAG,wCAAU,cAAC,IAAD,qBAAV,8BACH,uBArBJ,yKAwBI,uBAAM,uBACN,4BAAG,qDAAuB,cAAC,IAAD,2BAAvB,4BAA+E,cAAC,IAAD,wBAA/E,SACH,uBA1BJ,iJA6BI,uBAAM,uBA7BV,2UAmCA,uBACA,cAAC,IAAD,CAASzG,MAAO,CAACsB,eAAgB,MAAOC,eAAgB,UAAWC,QAAS,MAC5E,oBAAIV,GAAG,SAASN,UAAU,gBAA1B,8BAGA,oBAAIM,GAAG,OAAON,UAAU,gBAAxB,0BAGA,ybAMI,uBAAM,uBANV,olBAcA,uBACA,oBAAIM,GAAG,cAAcN,UAAU,gBAA/B,4BAGA,+FACkE,cAAC,IAAD,kBADlE,yJAGU,cAAC,IAAD,kBAHV,gKAKI,uBAAM,uBALV,6vBAa6D,0CAb7D,iUAkBA,uBACA,oBAAIM,GAAG,aAAaN,UAAU,gBAA9B,8BAGA,ggBAI8E,uCAJ9E,iIAMI,uBAAM,uBANV,6QAUI,uBAAM,uBAVV,mhBAgBgC,cAAC,IAAD,kBAhBhC,aAiBI,uBAAM,uBACN,sBAAKR,MAAO,CAAC4G,YAAa,MAAOC,aAAc,MAAOX,UAAW,UAAjE,UACI,cAAC,IAAD,0DADJ,eAKI,cAAC,IAAD,8CALJ,eASI,cAAC,IAAD,uCAKR,uBACA,oBAAIpF,GAAG,cAAcN,UAAU,gBAA/B,6BAGA,4XAI+C,cAAC,IAAD,wBAJ/C,yNAM6D,wCAN7D,kOAQsF,cAAC,IAAD,4BARtF,sNAYA,qBAAKR,MAAO,CAAC8G,OAAQ,SAAUrC,cAAe,OAAQ/D,MAAO,OAAQqG,SAAU,SAA/E,SACI,cAAC,IAAD,CAAgBN,IAAKO,MAEzB,uBACA,oBAAIlG,GAAG,cAAcN,UAAU,gBAA/B,0BAGA,8BACI,4BAAG,mEACH,uBAFJ,4PAMI,uBAAM,uBACN,4BAAG,uEACH,uBARJ,sMAWI,uBAAM,uBACN,4BAAG,iDACH,uBAbJ,+ZAmBA,uBACA,cAAC,IAAD,CAASR,MAAO,CAACsB,eAAgB,MAAOC,eAAgB,UAAWC,QAAS,MAC5E,oBAAIV,GAAG,eAAeN,UAAU,gBAAhC,wCAGA,6IAGA,qBAAKR,MAAO,CAAC8G,OAAQ,SAAUrC,cAAe,OAAQ/D,MAAO,OAAQqG,SAAU,SAAUd,gBAAiB,SAA1G,SACI,qBAAKQ,IAAKQ,MAEd,uBACA,qBAAKzG,UAAU,eAAeR,MAAO,CAACU,MAAO,QAA7C,SACI,cAAC,IAAD,CACIwG,SAAS,SACTC,iBAAiB,EACjBnH,MAAOoH,IACPC,mBAAoB,EAJxB,SEvTN,86BFgUE,uBACA,cAAC,IAAD,CAASrH,MAAO,CAACsB,eAAgB,MAAOC,eAAgB,UAAWC,QAAS,MAC5E,oBAAIV,GAAG,UAAUN,UAAU,gBAA3B,qBAGA,oBAAIM,GAAG,YAAYN,UAAU,gBAA7B,uBAGA,yDAEI,uBAAM,uBACN,4BAAG,yCACH,uBAJJ,SAKU,cAAC,IAAD,qBALV,iJAOI,uBAAM,uBACN,4BAAG,mDACH,uBATJ,8CAU+C,oCAV/C,IAWI,uBAAM,uBACN,4BAAG,kDACH,uBAbJ,oDAcqD,mCAdrD,IAeI,uBAAM,uBACN,4BAAG,wCACH,uBAjBJ,iBAkBkB,yCAlBlB,6DAoBA,uBACA,oBAAIM,GAAG,QAAQN,UAAU,gBAAzB,mCAGA,khBAMI,uBAAM,uBACN,qBAAKR,MAAO,CAAC8G,OAAQ,SAAUrC,cAAe,OAAQ/D,MAAO,OAAQqG,SAAU,SAA/E,SACI,cAAC,IAAD,CAAgBN,IAAKa,MAEzB,uBAVJ,oPAcI,uBAAM,uBAdV,2rBAuBI,uBAAM,uBAvBV,0VA4BI,uBAAM,uBA5BV,m3BAsCI,uBAAM,uBAtCV,gFAwCI,uBAAM,uBACN,qBAAKtH,MAAO,CAAC8G,OAAQ,SAAUrC,cAAe,OAAQ/D,MAAO,OAAQqG,SAAU,SAA/E,SACI,cAAC,IAAD,CAAgBN,IAAKc,MAEzB,uBA5CJ,yCA8CI,uBAAM,uBACN,qBAAKvH,MAAO,CAAC8G,OAAQ,SAAUrC,cAAe,OAAQ/D,MAAO,OAAQqG,SAAU,SAA/E,SACI,cAAC,IAAD,CAAgBN,IAAKe,SAG7B,uBACA,oBAAI1G,GAAG,WAAWN,UAAU,gBAA5B,8BAGA,qBAAKR,MAAO,CAACyE,cAAe,OAAQyB,UAAW,UAA/C,SACI,uBAAOlG,MAAO,CAACqG,UAAW,QAAS3F,MAAO,QAAU4F,UAAQ,EAACC,MAAI,EAACC,OAAK,EAAvE,SACI,wBAAQC,IAAKgB,EAAkBd,KAAK,kBAG5C,uBACA,oBAAI7F,GAAG,eAAeN,UAAU,gBAAhC,0BAGA,uZAKI,uBAAM,uBALV,8VAUI,uBAAM,uBAVV,+nBAiBI,uBAAM,uBAjBV","file":"static/js/15.29c430b2.chunk.js","sourcesContent":["import React, { useState, useEffect } from 'react';\r\nimport { Link, useLocation } from 'react-router-dom';\r\nimport { Divider, Tooltip, Image } from 'antd';\r\nimport { GithubOutlined, SendOutlined } from '@ant-design/icons';\r\n\r\nimport \"../css/projectpage.scss\";\r\n\r\nconst HomeTemplate = ({title, githubURL, projectRoute, projectLink, thumb}) => {\r\n    const loc = useLocation().pathname;\r\n\r\n    useEffect(() => {\r\n        window.scrollTo(0, 0)\r\n      }, [])\r\n\r\n    return (\r\n        <>\r\n        <div style={{\r\n            marginTop: \"-3rem\",\r\n            backgroundImage: `url(${thumb})`,\r\n            backgroundPosition: 'center',\r\n            backgroundSize: 'cover',\r\n            backgroundRepeat: 'no-repeat',\r\n            height: '100vh',\r\n            zIndex: -1,\r\n        }} />\r\n        <div className=\"project-home-wrapper\" style={{position: \"absolute\", width: \"100%\", top: \"101vh\", left: \"0px\", transform: \"translate(0, -100%)\"}}>\r\n            <header className=\"home-header\">\r\n                <h1 id=\"title\" style={{display: 'inline-block'}}>{title}</h1>\r\n                <span style={{padding: \"0 1em\", display: 'inline-block'}}>\r\n                    {githubURL != undefined ? (\r\n                        <Tooltip title=\"View on Github\" placement=\"bottom\">\r\n                            <a href={githubURL} target=\"_blank\">\r\n                                <GithubOutlined className=\"title-icon\"/>\r\n                            </a>\r\n                        </Tooltip>\r\n                    ) : null}\r\n\r\n                    {projectRoute != undefined  ? (\r\n                        <Tooltip title=\"View project\" placement=\"bottom\">\r\n                            <Link to={loc + projectRoute}>\r\n                                <SendOutlined className=\"title-icon\"/>\r\n                            </Link>\r\n                        </Tooltip>\r\n                    ) : null}\r\n\r\n                    {projectLink != undefined  ? (\r\n                        <Tooltip title=\"View project\" placement=\"bottom\">\r\n                            <a href={projectLink} target=\"_blank\">\r\n                                <SendOutlined className=\"title-icon\"/>\r\n                            </a>\r\n                        </Tooltip>\r\n                    ) : null}\r\n\r\n                </span>\r\n                <Divider style={{borderTopWidth: \"1px\", borderTopColor: \"#000000\", opacity: 0.5}}/>\r\n            </header>\r\n            <div style={{height: \"8em\"}}>\r\n\r\n            </div>\r\n        </div>\r\n        </>\r\n    );\r\n}\r\n\r\nexport default HomeTemplate;","\r\nimport { Breadcrumb, Anchor } from 'antd';\r\nimport { HashLink } from 'react-router-hash-link';\r\nimport React, { useState, useEffect, useRef } from 'react';\r\n\r\nconst useHeadingsData = () => {\r\n\tconst [nestedHeadings, setNestedHeadings] = useState([]);\r\n\r\n\tuseEffect(() => {\r\n\t\tconst headingElements = Array.from(\r\n\t\t\tdocument.querySelectorAll(\"h1, h2, h3\")\r\n\t\t);\r\n\r\n\t\t// Created a list of headings, with H3s nested\r\n\t\tconst newNestedHeadings = getNestedHeadings(headingElements);\r\n\r\n\t\tsetNestedHeadings(newNestedHeadings);\r\n\t}, []);\r\n\r\n\treturn { nestedHeadings };\r\n};\r\n\r\nconst getNestedHeadings = (headingElements) => {\r\n\tvar headingAttention = headingElements.filter(e => {\r\n\t\tlet l = e.nodeName;\r\n\t\treturn e.id !== \"title\" && (l === \"H1\" || l === \"H2\" || l === \"H3\");\r\n\t})\r\n\r\n\tlet ki = -1;\r\n\tvar headingLst = headingAttention.map(e => {\r\n\t\tki++;\r\n\t\treturn ({ key: `contents_${ki}`, href: `#${e.id}`, title: e.innerHTML, level: parseInt(e.nodeName.slice(-1)) })\r\n\t})\r\n\tvar result = [],\r\n\t\tindices = [],\r\n\t\tlevels = [result]\r\n\r\n\theadingLst.forEach(o => {\r\n\t\tvar index = indices.findIndex(level => level >= o.level);\r\n\t\tif (index === -1) {\r\n\t\t\tindex = indices.push(o.level) - 1;\r\n\t\t} else {\r\n\t\t\tindices.length = index + 1;\r\n\t\t}\r\n\t\tlevels[index].push(Object.assign({}, o, { children: levels[index + 1] = [] }));\r\n\t});\r\n\r\n\r\n\treturn result;\r\n};\r\n\r\nconst useNavHeight = () => {\r\n\tconst [navHeight, setNavHeight] = useState(0);\r\n\r\n\tuseEffect(() => {\r\n\t\tconst navbar = document.getElementById(\"main-navbar\");\r\n\t\tconsole.log(navbar)\r\n\r\n\t\tsetNavHeight(navbar.offsetHeight);\r\n\t}, []);\r\n\r\n\treturn { navHeight };\r\n}\r\n\r\nconst TableOfContents = ({ title }) => {\r\n\tconst { nestedHeadings } = useHeadingsData();\r\n\tconst { navHeight } = useNavHeight();\r\n\r\n\tconst [anchorHeight, setAnchorHeight] = useState(\"85vh\");\r\n\r\n\tuseEffect(() => {\r\n\t\tconst breadcrumbRef = document.getElementById(\"toc-breadcrumb\");\r\n\t\tif (breadcrumbRef) {\r\n\t\t\tsetAnchorHeight(`calc(100vh - 6rem - ${breadcrumbRef.offsetHeight}px)`)\r\n\t\t}\r\n\t}, [])\r\n\r\n\treturn (\r\n\t\t<>\r\n\t\t\t<Breadcrumb id=\"toc-breadcrumb\" style={{paddingBottom: \"14px\", position: \"sticky\"}}\r\n\t\t\t\titems={[\r\n\t\t\t\t\t{\r\n\t\t\t\t\t\ttitle: <a href=\"#home\">Portfolio</a>,\r\n\t\t\t\t\t},\r\n\t\t\t\t\t{\r\n\t\t\t\t\t\ttitle: <a href=\"#projects\">Projects</a>,\r\n\t\t\t\t\t},\r\n\t\t\t\t\t{\r\n\t\t\t\t\t\ttitle: `${title}`\r\n\t\t\t\t\t}\r\n\t\t\t\t]}\r\n\t\t\t/>\r\n\t\t\t<Anchor\r\n\t\t\t\tstyle={{maxHeight: anchorHeight, overflow: \"auto\"}}\r\n\t\t\t\ttargetOffset={navHeight}\r\n\t\t\t\tonClick={(e, l) => {\r\n\t\t\t\t\t// I have no idea why this works in hash router link... but sure\r\n\t\t\t\t\te.preventDefault();\r\n\t\t\t\t}}\r\n\t\t\t\titems={nestedHeadings}\r\n\t\t\t/>\r\n\t\t</>\r\n\t);\r\n};\r\n\r\nexport default TableOfContents","import FadeIn from 'react-fade-in';\r\nimport HomeTemplate from './homeTemplate';\r\nimport TableOfContents from './tableofcontents';\r\nimport { Row, Col, Grid } from 'antd';\r\nconst { useBreakpoint } = Grid\r\n\r\nconst ProjectPage = ({title, thumb, projectLink, projectRoute, githubURL, footer, children}) => {\r\n    const screens = useBreakpoint();\r\n\r\n    return (\r\n    \r\n        <>\r\n        <FadeIn>\r\n            <HomeTemplate \r\n                title={title} \r\n                thumb={thumb} \r\n                projectRoute={projectRoute} \r\n                projectLink={projectLink} \r\n                githubURL={githubURL} />\r\n            <Row gutter={0}>\r\n            <Col xs={0} lg={5}>\r\n            <div className='project-toc-wrapper'>\r\n                <TableOfContents title={title}/> \r\n            </div>\r\n            </Col>\r\n            <Col xs={24} lg={19}>\r\n            <div className=\"project-content-wrapper\" style={{marginRight: screens.lg? \"17.5vw\" : \"6vw\", marginLeft: screens.lg? 0 : \"6vw\"}}>\r\n                {children}\r\n            </div>\r\n            <div className='project-footer-wrapper' style={{\r\n                display: \"flex\", justifyContent: \"center\", \r\n                marginTop: \"8vh\", marginBottom: \"5vh\",\r\n                marginRight: screens.lg? \"17.5vw\" : \"6vw\", marginLeft: screens.lg? 0 : \"6vw\"\r\n                }}>\r\n                {footer ? {footer} : \"❋ That's all! Thanks for reading. ❋\"}\r\n            </div>\r\n            </Col>\r\n            </Row>\r\n        </FadeIn>\r\n        </>\r\n    );\r\n}\r\n\r\nexport default ProjectPage;","import React from 'react'\r\nimport {Image} from 'antd'\r\n\r\nconst AnnotatedImage = ({annotation, fontSize, ...props}) => {\r\n\r\n    const paddingBottom = props.paddingBottom ? props.paddingBottom : \"20px\"\r\n\r\n    return (\r\n        <div style={{position: \"relative\"}}>\r\n            <Image {...props} />\r\n            {annotation ? (\r\n                <div className=\"styled-text\" style={{\r\n                    position: \"absolute\", \r\n                    bottom: 0, \r\n                    left: 0, \r\n                    backgroundColor: 'rgba(21, 25, 31, 0.65)',\r\n                    width: \"100%\", \r\n                    fontSize: fontSize && fontSize,\r\n                    textAlign: 'center',\r\n                    padding: \"10px 5px\",\r\n                    paddingBottom: paddingBottom\r\n                }}>\r\n                    {annotation}\r\n                </div>\r\n            ) : null}\r\n        </div>\r\n    );\r\n}\r\n\r\nexport default AnnotatedImage;","export default __webpack_public_path__ + \"static/media/Evaluating_EP56.71ddb9eb.mp4\";","export default __webpack_public_path__ + \"static/media/Rclip_Comparison.1e29e8cb.png\";","export default __webpack_public_path__ + \"static/media/216x64xn.d5547d33.svg\";","export default __webpack_public_path__ + \"static/media/compare_dqn.db755112.png\";","export default __webpack_public_path__ + \"static/media/STACK_DDQN_3.44bb37f1.png\";","export default __webpack_public_path__ + \"static/media/STACK_DDQN_3 (EVAL_3600).212306fc.png\";","export default __webpack_public_path__ + \"static/media/training_montage.5d359d49.mp4\";","import React from 'react';\r\nimport { HashLink } from 'react-router-hash-link';\r\nimport { Row, Col, Divider, Image } from 'antd';\r\nimport SyntaxHighlighter from 'react-syntax-highlighter';\r\nimport { dracula } from 'react-syntax-highlighter/dist/esm/styles/hljs';\r\n\r\nimport racing_example from '../../content/projects/CarRacing/Evaluating_EP56.mp4'\r\nimport preprocess from '../../content/projects/CarRacing/preprocessing/preprocessing_steps.png'\r\n\r\nimport rclip_compare from '../../content/projects/CarRacing/graphs/Rclip_Comparison.png'\r\nimport architecture from '../../content/projects/CarRacing/graphs/216x64xn.svg'\r\nimport architecture_code from '../../content/projects/CarRacing/code_snippets/architecture'\r\n\r\nimport compare_dqn from '../../content/projects/CarRacing/graphs/compare_dqn.png'\r\nimport final_dqn from '../../content/projects/CarRacing/graphs/STACK_DDQN_3.png'\r\nimport final_dqn_hist from '../../content/projects/CarRacing/graphs/STACK_DDQN_3 (EVAL_3600).png'\r\n\r\nimport training_montage from '../../content/projects/CarRacing/training_montage.mp4'\r\n\r\nimport 'katex/dist/katex.min.css'\r\nimport Latex from 'react-latex-next';\r\n\r\nimport Meta from '.';\r\nimport ProjectPage from '../projectPage';\r\nimport AnnotatedImage from '../annotatedImage';\r\n\r\nconst Home = () => {\r\n    return (\r\n    \r\n        <ProjectPage title={Meta.title} thumb={Meta.thumb}>\r\n            <h1 id=\"overview\" className=\"raleway-title\">\r\n                Overview\r\n            </h1>\r\n            <div style={{paddingBottom: \"20px\", textAlign: \"center\"}}>\r\n                <video style={{objectFit: \"cover\", width: \"100%\"}}  autoPlay loop muted>\r\n                    <source src={racing_example} type='video/mp4' />\r\n                </video>\r\n            </div>\r\n            <p>\r\n                Reinforcement learning (RL) is a subfield of machine learning that focuses on training agents\r\n                to make sequential decisions in an environment to maximize a cumulative reward. \r\n                Unlike supervised learning, where labeled examples are used to directly learn a mapping between inputs and outputs,\r\n                reinforcement learning involves an agent interacting with an environment, learning from feedback signals\r\n                (rewards or penalties) to improve its decision-making abilities over time.\r\n                <br /><br />\r\n                In this post, I provide a short walkthrough on how to train a reinforcement learning agent to tackle OpenAI's CarRacing domain, the results of which can be seen above.\r\n                OpenAI's Car Racing Environment serves as an excellent benchmark for testing the capabilities of reinforcement learning algorithms. \r\n                An agent in this environment must train to recognise patterns from pixel data, and learn to control a physically-simulated car according to what it sees.\r\n                The dimensionality of this domain is somewhat high so we cannot apply brute-force learning methods, and instead must make use of approximation. Neural Networks are an\r\n                amazing tool for this, and have paved the way for many breakthroughs in the field of RL within the last decade. For example, two you may be familiar with: AlphaGo and OpenAI Five.\r\n            </p>\r\n            <br />\r\n            <Divider style={{borderTopWidth: \"1px\", borderTopColor: \"#000000\", opacity: 0.5}}/>\r\n            <h1 id=\"rl-into\" className=\"raleway-title\">\r\n                A Brief intro to RL\r\n            </h1>\r\n            <p>\r\n                The RL problem involves an agent interacting with an environment, where the agent takes actions\r\n                based on its observations, receives feedback in the form of rewards, and learns to improve its decision-making \r\n                abilities as it trains. The problem can be formalized as a Markov Decision Process (MDP), which consists of a set of states, \r\n                actions, transition probabilities, and rewards. The goal of the RL agent is to learn an optimal \r\n                policy - a mapping from states to actions - that maximizes the cumulative reward it receives over time in the environment.\r\n                <br /><br />\r\n                We encounter a different set of machine learning problems with RL. \r\n                For instance, our reward function can be sparse, making optimisation tricky. \r\n                Additionally, an agent has to consider a lot of unknowns; it does not ncessarily know what the result of taking a specific action\r\n                may be - it must learn from experience. This introduces a trade-off between exploration: trying out new actions to learn more about the environment, and exploitation: \r\n                choosing actions that are known to yield higher rewards.\r\n            </p>\r\n            <br />\r\n            <Divider style={{borderTopWidth: \"1px\", borderTopColor: \"#000000\", opacity: 0.5}}/>\r\n            <h1 id=\"dqn\" className=\"raleway-title\">\r\n                Deep Q Learning (DQN)\r\n            </h1>\r\n            <p>\r\n                DQN is a friendly starting point for training an RL agent due to its simplicity and versatile training process. DQN involves learning a 'Q-value' for each state-action pair which \r\n                is an estimate of the expected reward an agent would receive for taking a particular action in a particular state. An agent can then choose sensible actions by greedily selecting actions which lead to the highest\r\n                expected reward.\r\n                <br /><br />\r\n                Two observations we can make from this explanation are 1: since we have state-action pairs, we must be working with a discrete action space. And 2: the size of the state-action pair space\r\n                is likely very large, or infinite for more challenging problems. This is where the 'deep' part of Deep-Q comes from. We cannot possibly generate an optimal set of Q-values for an infinitely large space, \r\n                so we make use of deep learning to approximate this Q-value function for all states. DQN is infact based off of an algorithm which indeed tabulates all possible Q-values, named Q-learning, but this is obviously \r\n                not applicable to this more complex domain.\r\n                <br /><br />\r\n                An important consideration in the CarRacing domain is how to deal with the large observation space we have. The \r\n                observation of pixel data is a <Latex>{`$96\\\\times96$`}</Latex> matrix (at a minimum, 9216 values!), which would make training slow for a regular\r\n                dense network. The obvious choice instead is to use a convolutional neural network (CNN) to reduce the input dimensions and to learn spatial-features (e.g. distinguishing track from off-road) before \r\n                passing information to a dense network.\r\n            </p>\r\n            <br />\r\n            <h1 id=\"ddqn\" className=\"raleway-title\">\r\n                Double Deep Q (DDQN)\r\n            </h1>\r\n            <p>\r\n                DQN, as introduced above, is a method which is able to learn an optimal policy through\r\n                approximation of Q-values (from ideas in tabular Q-learning). Q-values describe the expected return\r\n                from taking a certain action in a certain state. This allows an agent to make decisions in its current state\r\n                without having to look forward in time, which is often not possible. An agent can therefore greedily\r\n                select actions with the highest Q-values in order to follow it's current best policy. A deep model is\r\n                used to generate approximate Q-values from a state-observation. Policies are refined by reducing\r\n                loss calculated differences in current vs. expected q-value based on the reward received from many\r\n                state-transitions (experiences).\r\n                <br /><br />\r\n                DDQN expands on these ideas slightly by tackling a potent issue in DQN: overestimation bias. Since\r\n                an agent greedily follows its best policy to evaluate expected return, it is likely to make an overestimate\r\n                - it is only concerned with the best possible outcomes in the future. In actuality, not all outcomes will\r\n                be 'optimal' and so the value is higher than expected. This issue is additionally worsened when the frequency of estimations is increased.\r\n                <br /><br />\r\n                DDQN reduces overestimation bias by maintaining two networks; an online 'training' network for\r\n                action selection, and an offline 'target' network for q-value estimation. The target network is updated\r\n                to match the training network periodically, thus stays reasonably stable for q-value estimation and\r\n                reduces overestimation. This changes the loss calculation under MSE for non-terminal <Latex>{`$s'$`}</Latex> from\r\n                <br /><br />\r\n                <div style={{paddingLeft: \"3em\", paddingRight: \"3em\", textAlign: \"center\"}}>\r\n                    <Latex>\r\n                        {`$L(s,a,s',r)=\\\\bigg(\\\\big(r+\\\\gamma\\\\max_{a'}Q(s',a')\\\\big)-Q(s,a)\\\\bigg)^2$`}\r\n                    </Latex>\r\n                </div>\r\n                <br />\r\n                to\r\n                <br /><br />\r\n                <div style={{paddingLeft: \"3em\", paddingRight: \"3em\", textAlign: \"center\"}}>\r\n                    <Latex>\r\n                        {`$L(s,a,s',r)=\\\\bigg(\\\\underbrace{\\\\big(r+\\\\gamma\\\\max_{a'}Q_{\\\\text{target}}(s',a')\\\\big)}_{\\\\text{new q-value estimate } \\\\hat q}-Q_{\\\\text{train}}(s,a)\\\\bigg)^2$`}\r\n                    </Latex>\r\n                </div>\r\n                (for terminal <Latex>{`$s'$`}</Latex>, DDQN still uses <Latex>{`$\\\\hat q = r$`}</Latex>)\r\n            </p>\r\n            <br />\r\n            <Divider style={{borderTopWidth: \"1px\", borderTopColor: \"#000000\", opacity: 0.5}}/>\r\n            <h1 id=\"preprocessing\" className=\"raleway-title\">\r\n                Preprocessing\r\n            </h1>\r\n            <p>\r\n                The default state space provided by the environment contains a lot of information which is effectively\r\n                redundant to an agent. In fact, a lot of this information makes training the agent <i>significantly harder</i> as\r\n                it introduces a lot of noise into the agent's observation while providing negligible meaningful content.\r\n            </p>\r\n            <div style={{margin: \"0 auto\", paddingBottom: \"20px\", width: \"100%\"}}>\r\n                <AnnotatedImage width=\"100%\" src={preprocess}/>\r\n            </div>\r\n            <p>\r\n                The exact steps are as follows:\r\n                <br /><br />\r\n                <b><u>1. Bottom UI Bar Removed.</u></b>\r\n                <br />\r\n                Mostly is noise. Contains visualisations of the agents score, driving speed and turning speeds, but\r\n                information is hard to extract from this due to pooling layers.\r\n                <br /><br />\r\n                <b><u>2. Billinear downscaling replaced by nearest-neighbour downscaling.</u></b>\r\n                <br />\r\n                Billinear downscaling introduces unnecessary noise (blurring) along the road boundary and around\r\n                the car's position. It also causes the number of unique colour values in the observation to increase\r\n                which may make mapping features more difficult.\r\n                <br /><br />\r\n                <b><u>3. Green grass pixels clamped to white.</u></b>\r\n                <br />\r\n                Further removal of unecessary information; the agent only needs to know where the grass boundary\r\n                is. It generally should not be going into the grass as this has no positive reward. Clamping to white\r\n                is sufficient, and makes the boundary more distinct from other features in the observation.\r\n                <br /><br />\r\n                <b><u>4. RGB <Latex>{`$\\\\to$`}</Latex> Grayscale conversion.</u></b>\r\n                <br />\r\n                Cuts the observation size down to a third without losing important information; all key features (road,\r\n                grass, checkpoints, and car) are still unique colours in mono.\r\n                <br /><br />\r\n                <b><u>5. Grayscale values <Latex>{`$\\\\in[0, 1]$`}</Latex> normalised to the range <Latex>{`$[-1, 1]$`}</Latex>.</u></b>\r\n                <br />\r\n                A generally useful trick for training neural networks. Helps reduce vanishing and exploding gradients,\r\n                and smooths the optimisation landscape.\r\n                <br /><br />\r\n                Additionally, some undesired behaviour of the environment was also removed, such as the view of the\r\n                track zooming in within the first few frames; this would make the agent's observation fundamentally\r\n                different within the first few frames and subsequently harder to learn from or generalise the rest of\r\n                its racing strategy to.\r\n            </p>\r\n            <br />\r\n            <Divider style={{borderTopWidth: \"1px\", borderTopColor: \"#000000\", opacity: 0.5}}/>\r\n            <h1 id=\"tweaks\" className=\"raleway-title\">\r\n                Algorithm Tweaks\r\n            </h1>\r\n            <h2 id=\"acsp\" className=\"raleway-title\">\r\n                Action Space\r\n            </h2>\r\n            <p>\r\n                By default, the environment operates under continuous actions. This is suitable for PPO, however\r\n                DDQN operates only on discrete action spaces. As such, the action space is reduced to a set of 5\r\n                primary actions for DDQN models which capture the main extents of which the car can be controlled:\r\n                full left & right turning, full acceleration, and soft braking (full braking only is too harsh for smooth\r\n                control).\r\n                <br /><br />\r\n                Additionally, training can be quite slow from the offset as the car starts from a standstill. The only\r\n                useful action an agent can take in this state is acceleration; all others keep the car stuck in the standstill.\r\n                Therefore a parameter can be given to bias acceleration actions when choosing a random action. For\r\n                instance, the probability distribution for the reduced action space under an acceleration bias factor\r\n                4 would be [0.125, 0.125, 0.5, 0.125, 0.125]. This helps speed up the start of training by increasing the\r\n                change of generating useful experiences from the offset of training.\r\n            </p>\r\n            <br />\r\n            <h2 id=\"frame-stack\" className=\"raleway-title\">\r\n                Frame Stacking\r\n            </h2>\r\n            <p>\r\n                Frame skipping refers to allowing the agent to only act every <Latex>{`$N$`}</Latex> frames, and having it repeat the same\r\n                action in the between-frames. On its own, this allows for an agent to train faster as it will only train\r\n                every <Latex>{`$N$`}</Latex> frames; training on frame data is expensive and it is more important that diverse experiences\r\n                are gathered rather than large volumes for effective learning.\r\n                <br /><br />\r\n                Frame stacking refers to aggregating multiple sequentially observed frames into a single observation.\r\n                The creators of the technique saw success with it when playing Atari games, to which this domain is\r\n                similar. Use of the aforementioned between-frames from frame skipping is an easy way of creating\r\n                such a frame stack, i.e. combining a number of 2D observations (frames) into a single 3D observation.\r\n                This increases the observation space substantially, but is manageable to train on using convolution\r\n                and pooling. Frame stacking is very beneficial however due to the fact that it adds previously absent\r\n                temporal information into the observation. This allows an agent to gauge information about its speed,\r\n                or turning speed, which is not possible without. This is <i>extremely</i> important for making tight turns and\r\n                preparing ahead, as prepration for a turn really begins some time before hitting the apex. Unreliable\r\n                preparation for turns is a major problem without frame stacking, and perhaps the single most important\r\n                feature for increased general performance based on experimental results.\r\n            </p>\r\n            <br />\r\n            <h2 id=\"batch-prio\" className=\"raleway-title\">\r\n                Priority Batches\r\n            </h2>\r\n            <p>\r\n                By default, experience replay batches are uniformly sampled from the agent's memory buffer. This\r\n                ensures the agent learns from all memories currently in the buffer given enough updates. This approach\r\n                is generally fine, but when working on a reasonably complex environment (especially with limited computing power), learning like this can be quite slow. An agent can accumulate a fair amount of experience\r\n                - especially with a large replay buffer - but not actually learn from the <i>useful</i> experience it has gathered\r\n                (e.g. major control mistakes), as batches get diluted with rudimentary errors from early training.\r\n                <br /><br />\r\n                Prioritised Experience Replay (PER) has been proposed to ensure agents learn\r\n                from the most salient errors in their gathered experience. Samples are importance sampled using the\r\n                TD-error in the DDQN update step as the sampling probability distribution function (pdf).\r\n                <br /><br />\r\n                Taking ideas from the above, a simplified pdf can be defined under the assumption that, generally\r\n                speaking, more recently gathered experiences are more useful to learn from as it is more likely the\r\n                agent has a decent level of control over the car in these experiences as training progresses. This\r\n                can potentially help the agent to learn a more optimal strategy but its main purpose is for reducing\r\n                convergence times. A simple linear function over the indices of items in the replay buffer works well\r\n                (derived from Gauss' sum to <Latex>{`$n$`}</Latex> numbers):\r\n                <br /><br />\r\n                <div style={{paddingLeft: \"3em\", paddingRight: \"3em\", textAlign: \"center\"}}>\r\n                    <Latex>\r\n                        {`$\\\\displaystyle pdf(i)=\\\\frac{2i}{N(N+1)},$`}\r\n                    </Latex>\r\n                    &nbsp;&nbsp;&nbsp;\r\n                    <Latex>\r\n                        {`$N=len(\\\\text{Replay Buffer}),$`}\r\n                    </Latex>\r\n                    &nbsp;&nbsp;&nbsp;\r\n                    <Latex>\r\n                        {`$i\\\\in [1,\\\\ N+1]$`}\r\n                    </Latex>\r\n                </div>\r\n            </p>\r\n            <br />\r\n            <h2 id=\"reward-clip\" className=\"raleway-title\">\r\n                Reward Clipping\r\n            </h2>\r\n            <p>\r\n                Agents have a tendency to act greedily to accumulate reward, and this is especially noticeable for\r\n                short-term when the car is moving quickly. High rewards can be accumulated in a single frame when the\r\n                car crosses multiple checkpoints at once due to high speeds. On subsequent iterations, these translate\r\n                into high short-term rewards (only minorly <Latex>{`$\\\\gamma$`}</Latex>-discounted) and encourage reckless driving tactics, seen\r\n                mostly as extremely high speeds on straightaways. Cornering is near impossible at these speeds and\r\n                is a major case of failure. Subsequently, rewards can be <i>clipped</i> - 'truncated' so to speak - so no more\r\n                than a little reward can be accumulated each frame. This implicitly discourages greedy behaviour. In\r\n                this implementation, cumulative rewards from frame-skips are clamped to the range <Latex>{`$[-10, \\\\ 1]$`}</Latex>. \r\n                The graph below shows experimentally that reward clipping can help training converge more quickly - almost twice\r\n                as quickly in this example (~400 episode convergence with; ~800 episode convergence without).\r\n            </p>\r\n            <div style={{margin: \"0 auto\", paddingBottom: \"20px\", width: \"100%\", maxWidth: \"574px\"}}>\r\n                <AnnotatedImage src={rclip_compare}/>\r\n            </div>\r\n            <br />\r\n            <h2 id=\"other-tweak\" className=\"raleway-title\">\r\n                Other Tweaks\r\n            </h2>\r\n            <p>\r\n                <b><u>Regularization for networks (L2)</u></b>\r\n                <br />\r\n                A general machine learning technique. Aims to help prevent overfitting and promote generalisation.\r\n                In this case, deterring a network from relying on any particular node too much may prevent the agent\r\n                from learning unusual quasi-effective strategies.\r\n                <br /><br />\r\n                <b><u>Exponentially decaying learning rate</u></b>\r\n                <br />\r\n                Another general machine learning technique. Helps fine-tune a model after it has done significant\r\n                learning in the beginning of training - when major changes to policy are likely no longer needed.\r\n                <br /><br />\r\n                <b><u>Early Stopping</u></b>\r\n                <br />\r\n                Stopping episodes early when an agent is performing badly. This helps train the agent faster (episodes\r\n                are generally shorter) and to not dilute the agent's gathered experience with states it should not have\r\n                found itself in to begin with. This involves stopping episodes when cumulative reward drops below\r\n                a certain threshold (-10) or when the agent receives negative reward n frames in a row (hyperparameter).\r\n            </p>\r\n            <br />\r\n            <Divider style={{borderTopWidth: \"1px\", borderTopColor: \"#000000\", opacity: 0.5}}/>\r\n            <h1 id=\"architecture\" className=\"raleway-title\">\r\n                Final Network Architecture\r\n            </h1>\r\n            <p>\r\n                Below is a visualisation of the final CNN architecture used as well as its corresponding Tensorflow summary.\r\n            </p>\r\n            <div style={{margin: \"0 auto\", paddingBottom: \"20px\", width: \"100%\", maxWidth: \"1200px\", backgroundColor: \"white\"}}>\r\n                <img src={architecture}/>\r\n            </div>\r\n            <br />\r\n            <div className=\"code-snippet\" style={{width: \"100%\"}}>\r\n                <SyntaxHighlighter \r\n                    language=\"python\" \r\n                    showLineNumbers={false}\r\n                    style={dracula}\r\n                    startingLineNumber={0}\r\n                >\r\n                    {architecture_code}\r\n                </SyntaxHighlighter>\r\n            </div>\r\n            <br />\r\n            <Divider style={{borderTopWidth: \"1px\", borderTopColor: \"#000000\", opacity: 0.5}}/>\r\n            <h1 id=\"results\" className=\"raleway-title\">\r\n                Results\r\n            </h1>\r\n            <h2 id=\"baselines\" className=\"raleway-title\">\r\n                Baselines\r\n            </h2>\r\n            <p>\r\n                Firstly, some baselines:\r\n                <br /><br />\r\n                <b><u>Solved</u></b>\r\n                <br />\r\n                Score <Latex>{`$\\\\gt$`}</Latex> 900 for single episodes. The environment is considered fully solved when\r\n                an average score of 900 is achieved across 100 consecutive episodes.\r\n                <br /><br />\r\n                <b><u>Zero-performance</u></b>\r\n                <br />\r\n                Random Agent. Achieves an average score of <b>-10</b>.\r\n                <br /><br />\r\n                <b><u>Low-performance</u></b>\r\n                <br />\r\n                Forward Only Agent. Achieves an average score of <b>80</b>.\r\n                <br /><br />\r\n                <b><u>Human</u></b>\r\n                <br />\r\n                Anywhere from <b>low 100s</b> all the way up to full solves depending on experience.\r\n            </p>\r\n            <br />\r\n            <h2 id=\"graph\" className=\"raleway-title\">\r\n                Graphs and Discussion\r\n            </h2>\r\n            <p>\r\n                Overall, through use of DDQN, we can see promising and performative results. All agents exhibited\r\n                clear attempts to stay on track and drive at high speeds when possible. On the best trained model\r\n                (STACK_DDQN_3), the agent is close to full episode solves approximately 50% of the time. Additionally, \r\n                the agent consistently beat both baseline measures, even in episodes with relatively poor performance compared \r\n                to the max/mean. At it's best performance, it likely drives better than a human expert.\r\n                <br /><br />\r\n                <div style={{margin: \"0 auto\", paddingBottom: \"20px\", width: \"100%\", maxWidth: \"817px\"}}>\r\n                    <AnnotatedImage src={compare_dqn}/>\r\n                </div>\r\n                <br />\r\n                We can observe some interesting emergent behaviour from agents, including agents getting back on track\r\n                after failing (recovery), corner cutting without skipping checkpoints, and tight racing lines (especially\r\n                around the tightest of corners).\r\n                <br /><br />\r\n                Model evaluations indicate that mean, as well as maximum performance can be significantly\r\n                increased by introducing temporal data into models via framestacking. Without framestacking, agents\r\n                are not able to fully solve any episodes, and are prone to greedy and reckless driving behaviours.\r\n                Priority batch sampling an agent's experience during replay significantly reduced total convergence\r\n                time. A comparison of STACK_DDQN_P_1 and STACK_DDQN_3 indicates that priority batching\r\n                can reduce convergence times by approximately a half. The rationale for this improvement is that\r\n                latter agent experience is likely to be closer to good racing policy, and is therefore good to learn from\r\n                when fine-tuning.\r\n                <br /><br />\r\n                Moreover, a comparison of DDQN_P_1 and DDQN_P_2 indicates that reward clipping also\r\n                helped reduce convergence times and overall training consistency. Reward clipping helped reduce\r\n                overconfident driving, which was especially prominent in agents like the aforementioned which didn't\r\n                train on temporally-correlated information (no framestacking).\r\n                <br /><br />\r\n                One of the most potent issues with trained agents is inconsistency: the standard deviation about the\r\n                mean score is relatively high for all agents. This is partly due to the fact that the agents are punished\r\n                harshly for mistakes (via early episode termination) but also due to the fact that sometimes an agent\r\n                isn't able to perform a simple action which it is able to perform in a seemingly similar scenario. This\r\n                is most often a failed early turn. This could be due to three reasons: 1) some level of forgetting - an\r\n                agent dropping an older learned policy in favour of a new one based on new experience. 2) somewhat\r\n                incoherent models of the track in agents' convolutional layer feature maps. 3) agents sometimes being\r\n                forced to make suboptimal moves due to turning being somewhat heavy-handed in the discrete action\r\n                space. Such actions have to be repeated during frame skips also.\r\n                <br /><br />\r\n                Below is a detailed training graph for the best trained model (STACK_DDQN_3):\r\n                <br /><br />\r\n                <div style={{margin: \"0 auto\", paddingBottom: \"20px\", width: \"100%\", maxWidth: \"817px\"}}>\r\n                    <AnnotatedImage src={final_dqn}/>\r\n                </div>\r\n                <br />\r\n                And a histogram of evaluation results:\r\n                <br /><br />\r\n                <div style={{margin: \"0 auto\", paddingBottom: \"20px\", width: \"100%\", maxWidth: \"817px\"}}>\r\n                    <AnnotatedImage src={final_dqn_hist}/>\r\n                </div>\r\n            </p>\r\n            <br />\r\n            <h1 id=\"training\" className=\"raleway-title\">\r\n                Training Montage\r\n            </h1>\r\n            <div style={{paddingBottom: \"20px\", textAlign: \"center\"}}>\r\n                <video style={{objectFit: \"cover\", width: \"100%\"}}  autoPlay loop muted>\r\n                    <source src={training_montage} type='video/mp4' />\r\n                </video>\r\n            </div>\r\n            <br />\r\n            <h1 id=\"improvements\" className=\"raleway-title\">\r\n                Improvements\r\n            </h1>\r\n            <p>\r\n                First and foremost, it would be beneficial to allow the stacked DDQN models shown to train for more\r\n                iterations, ideally into the order of millions of training iterations. Current stacked models still show\r\n                shallow upwards trends when training was halted (especially STACK_DDQN_P_1), so it is possible\r\n                they would be able to solve the environment fully given enough iterations.\r\n                <br /><br />\r\n                It would be ideal to experiment further with different training approaches such as Dueling DQN.\r\n                Dueling DQN could alleviate some of the performance inconsistency\r\n                observed in late-stage training by assisting the agent in having a more coherent model of its environment\r\n                due to both training and target networks sharing the same convolutional layers.\r\n                <br /><br />\r\n                A different training avenue is under a continuous action space, using PPO. PPO has multiple benefits. PPO trains on continuous\r\n                action spaces, which would allow for better control in difficult scenarios, and would reduce the \r\n                heavy-handedness of turning overall. It is able to scale up well on hardware (multiple environments can be run\r\n                in parallel and save experiences to a shared buffer) which would be useful to cut down on the long training times \r\n                seen with DDQN. PPO using Actor-Critic based updates would hopefully reduce the amount\r\n                of hyperparameter tweaking which has to be done - this was a major time-sink during experimentation. \r\n                <br /><br />\r\n                Moving away from technical reinforcement learning improvements, a better baseline which could\r\n                be implemented is a PID controller - a handcrafted technique designed for autonomous control. This\r\n                technique finds use in automotive applications such as cruise control and RC racing. It would provide\r\n                a competitive baseline between the forward-only and human expert baselines.\r\n            </p>\r\n        </ProjectPage>\r\n    );\r\n}\r\n\r\nexport default Home;\r\n","export default \"data:image/png;base64,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\"","const Code = `\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\n Layer (type)                Output Shape              Param #\r\n=================================================================\r\n Convolutional_1 (Conv2D)    (None, 30, 30, 6)         300\r\n\r\n max_pooling2d (MaxPooling2D  (None, 15, 15, 6)        0\r\n )\r\n\r\n Convolutional_2 (Conv2D)    (None, 12, 12, 12)        1164\r\n\r\n max_pooling2d_1 (MaxPooling  (None, 6, 6, 12)         0\r\n 2D)\r\n\r\n flatten (Flatten)           (None, 432)               0\r\n\r\n Dense1 (Dense)              (None, 216)               93528\r\n\r\n Dense2 (Dense)              (None, 64)                13888\r\n\r\n dense (Dense)               (None, 5)                 325\r\n\r\n=================================================================\r\nTotal params: 109,205\r\nTrainable params: 109,205\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n`;\r\n\r\nexport default Code"],"sourceRoot":""}